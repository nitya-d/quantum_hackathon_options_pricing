{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba540e0",
   "metadata": {},
   "source": [
    "## QML Model — Level 1: Future Swaption Surface Prediction\n",
    "Architecture: Quantum Reservoir Computing (QRC) with MerLin\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "    1. Load & preprocess Level 1 data\n",
    "\n",
    "    2. Reduce 224 features → N via PCA (quantum circuits have mode limits)\n",
    "\n",
    "    3. Fixed quantum reservoir (MerLin) extracts non-linear features\n",
    "\n",
    "    4. Trainable classical linear readout predicts next-day's full 224-dim surface\n",
    "\n",
    "    5. Train, evaluate, save predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e025de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting merlinquantum\n",
      "  Downloading merlinquantum-0.3.1-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from merlinquantum) (2.10.0+cpu)\n",
      "Collecting perceval-quandela>=0.13.1 (from merlinquantum)\n",
      "  Downloading perceval_quandela-1.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpy>=2.2.6 (from merlinquantum)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from merlinquantum) (2.2.2)\n",
      "Collecting scikit-learn>=1.7.2 (from merlinquantum)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy~=1.12 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.14.0)\n",
      "Requirement already satisfied: scipy~=1.13 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.16.3)\n",
      "Requirement already satisfied: tabulate~=0.9 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (0.9.0)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (3.10.0)\n",
      "Collecting exqalibur~=1.1.0 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading exqalibur-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (430 bytes)\n",
      "Requirement already satisfied: multipledispatch<2 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (5.29.6)\n",
      "Collecting drawsvg>=2.0 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading drawsvg-2.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests<3 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (2.32.4)\n",
      "Requirement already satisfied: networkx<4,>=3.1 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (3.6.1)\n",
      "Collecting latexcodec<4 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading latexcodec-3.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (4.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (4.67.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->merlinquantum) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->merlinquantum) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (75.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (2025.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->merlinquantum) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy~=1.12->perceval-quandela>=0.13.1->merlinquantum) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->merlinquantum) (3.0.3)\n",
      "Downloading merlinquantum-0.3.1-py3-none-any.whl (197 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.0/197.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading perceval_quandela-1.1.0-py3-none-any.whl (432 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.8/432.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading drawsvg-2.4.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading exqalibur-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading latexcodec-3.0.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: drawsvg, numpy, latexcodec, exqalibur, scikit-learn, perceval-quandela, merlinquantum\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed drawsvg-2.4.1 exqalibur-1.1.1 latexcodec-3.0.1 merlinquantum-0.3.1 numpy-2.4.2 perceval-quandela-1.1.0 scikit-learn-1.8.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "538bc2f152164f748c71a036c3880c68",
       "pip_warning": {
        "packages": [
         "numpy",
         "sklearn"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install merlinquantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb8125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c218b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datasets import load_dataset\n",
    "\n",
    "import merlin as ML\n",
    "from merlin import LexGrouping, MeasurementStrategy, ComputationSpace\n",
    "from merlin.builder import CircuitBuilder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b974387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "N_PCA_COMPONENTS  = 16    # PCA: 224 → 16 (fits quantum mode limit)\n",
    "LOOKBACK          = 5     # Days of history per sample → input = 5×16 = 80\n",
    "N_MODES           = 16    # Quantum circuit modes (≤ 20 QPU hard limit)\n",
    "N_PHOTONS         = 4     # Photons in the register\n",
    "N_GROUPED_OUTPUTS = 32    # LexGrouping: compress Fock space → 32 features\n",
    "TRAIN_SPLIT       = 0.85\n",
    "EPOCHS            = 80\n",
    "LR                = 5e-4\n",
    "BATCH_SIZE        = 16\n",
    "DEVICE            = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca20dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Raw data shape: (494, 224)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 1. LOAD DATA\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\n",
    "    \"Quandela/Challenge_Swaptions\",\n",
    "    data_files=\"level-1_Future_prediction/train.csv\",\n",
    "    split=\"train\",\n",
    ")\n",
    "df = ds.to_pandas()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c != \"Date\"]\n",
    "raw_data = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cfe455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance: 100.0%\n",
      "X shape: (489, 80)  (5 days × 16 PCA = 80 features)\n",
      "Y shape: (489, 224)\n",
      "\n",
      "Train: 415 samples | Val: 74 samples\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2. PREPROCESS\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# MinMax scale to [0, 1] — required for angle encoding stability (MerLin docs)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(raw_data).astype(np.float32)\n",
    "\n",
    "# PCA: 224 → 16\n",
    "pca = PCA(n_components=N_PCA_COMPONENTS)\n",
    "data_pca = pca.fit_transform(data_scaled).astype(np.float32)\n",
    "\n",
    "# Re-scale PCA outputs to [0, 1] so the pre-compression Sigmoid stays well-behaved\n",
    "pca_scaler = MinMaxScaler()\n",
    "data_pca = pca_scaler.fit_transform(data_pca).astype(np.float32)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3. BUILD LOOKBACK WINDOWS\n",
    "# ─────────────────────────────────────────────\n",
    "# Input:  last LOOKBACK days of PCA surface, flattened → (80,)\n",
    "# Target: full 224-dim surface on the next day\n",
    "\n",
    "N = len(data_pca)\n",
    "X_list, Y_list = [], []\n",
    "for i in range(LOOKBACK, N):\n",
    "    window = data_pca[i - LOOKBACK:i].flatten()\n",
    "    X_list.append(window)\n",
    "    Y_list.append(data_scaled[i])\n",
    "\n",
    "X = np.array(X_list, dtype=np.float32)   # (N-LOOKBACK, 80)\n",
    "Y = np.array(Y_list, dtype=np.float32)   # (N-LOOKBACK, 224)\n",
    "\n",
    "print(f\"X shape: {X.shape}  ({LOOKBACK} days × {N_PCA_COMPONENTS} PCA = {X.shape[1]} features)\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 4. TRAIN / VAL SPLIT  (chronological — never shuffle time series)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "split    = int(len(X) * TRAIN_SPLIT)\n",
    "X_train  = torch.tensor(X[:split], device=DEVICE)\n",
    "Y_train  = torch.tensor(Y[:split], device=DEVICE)\n",
    "X_val    = torch.tensor(X[split:], device=DEVICE)\n",
    "Y_val    = torch.tensor(Y[split:], device=DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} samples | Val: {len(X_val)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dae83a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum layer Fock output size : 1820\n",
      "After LexGrouping              : 32\n"
     ]
    }
   ],
   "source": [
    "builder = CircuitBuilder(n_modes=N_MODES)\n",
    "builder.add_entangling_layer(trainable=True, name=\"U1\")\n",
    "builder.add_angle_encoding(\n",
    "    modes=list(range(N_MODES)),\n",
    "    name=\"input\",\n",
    "    scale=np.pi,\n",
    ")\n",
    "builder.add_rotations(trainable=True, name=\"theta\")\n",
    "builder.add_superpositions(depth=1, trainable=True)\n",
    "\n",
    "quantum_core = ML.QuantumLayer(\n",
    "    input_size=N_MODES,\n",
    "    builder=builder,\n",
    "    n_photons=N_PHOTONS,\n",
    "    measurement_strategy=MeasurementStrategy.probs(ComputationSpace.UNBUNCHED),\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantum layer Fock output size : {quantum_core.output_size}\")\n",
    "print(f\"After LexGrouping              : {N_GROUPED_OUTPUTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f8484e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total params     : 134,414\n",
      "Trainable params : 134,414\n"
     ]
    }
   ],
   "source": [
    "class QRCSwaption(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classical pre-compression: squeeze lookback window → quantum-compatible size\n",
    "        # Sigmoid ensures output stays in [0,1] for angle encoding\n",
    "        self.pre_compress = nn.Sequential(\n",
    "            nn.Linear(input_size, N_MODES),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Quantum feature extraction + Fock space grouping\n",
    "        self.quantum = nn.Sequential(\n",
    "            quantum_core,\n",
    "            LexGrouping(quantum_core.output_size, N_GROUPED_OUTPUTS),\n",
    "        )\n",
    "\n",
    "        # Classical readout with regularization\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(N_GROUPED_OUTPUTS, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pre_compress(x)   # (B, 80) → (B, 16)\n",
    "        x = self.quantum(x)        # (B, 16) → (B, 32)\n",
    "        return self.readout(x)     # (B, 32) → (B, 224)\n",
    "\n",
    "\n",
    "model = QRCSwaption(\n",
    "    input_size=N_PCA_COMPONENTS * LOOKBACK,\n",
    "    output_size=len(feature_cols),\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal params     : {total_params:,}\")\n",
    "print(f\"Trainable params : {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df942114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "  Epoch   1/80 | train MSE: 0.060733 | val MSE: 0.055263 | best val: 0.055263\n",
      "  Epoch   2/80 | train MSE: 0.049993 | val MSE: 0.040517 | best val: 0.040517\n",
      "  Epoch   3/80 | train MSE: 0.048880 | val MSE: 0.040350 | best val: 0.040350\n",
      "  Epoch   4/80 | train MSE: 0.047454 | val MSE: 0.037089 | best val: 0.037089\n",
      "  Epoch   5/80 | train MSE: 0.047037 | val MSE: 0.035763 | best val: 0.035763\n",
      "  Epoch   6/80 | train MSE: 0.046037 | val MSE: 0.030827 | best val: 0.030827\n",
      "  Epoch   7/80 | train MSE: 0.045698 | val MSE: 0.028653 | best val: 0.028653\n",
      "  Epoch   8/80 | train MSE: 0.044382 | val MSE: 0.027213 | best val: 0.027213\n",
      "  Epoch   9/80 | train MSE: 0.043724 | val MSE: 0.025808 | best val: 0.025808\n",
      "  Epoch  10/80 | train MSE: 0.043293 | val MSE: 0.024190 | best val: 0.024190\n",
      "  Epoch  11/80 | train MSE: 0.042734 | val MSE: 0.029266 | best val: 0.024190\n",
      "  Epoch  12/80 | train MSE: 0.042317 | val MSE: 0.025553 | best val: 0.024190\n",
      "  Epoch  13/80 | train MSE: 0.041715 | val MSE: 0.026559 | best val: 0.024190\n",
      "  Epoch  14/80 | train MSE: 0.041418 | val MSE: 0.020197 | best val: 0.020197\n",
      "  Epoch  15/80 | train MSE: 0.041275 | val MSE: 0.029002 | best val: 0.020197\n",
      "  Epoch  16/80 | train MSE: 0.041033 | val MSE: 0.021999 | best val: 0.020197\n",
      "  Epoch  17/80 | train MSE: 0.040312 | val MSE: 0.027729 | best val: 0.020197\n",
      "  Epoch  18/80 | train MSE: 0.040074 | val MSE: 0.023528 | best val: 0.020197\n",
      "  Epoch  19/80 | train MSE: 0.039802 | val MSE: 0.026732 | best val: 0.020197\n",
      "  Epoch  20/80 | train MSE: 0.039669 | val MSE: 0.024858 | best val: 0.020197\n",
      "  Epoch  21/80 | train MSE: 0.038805 | val MSE: 0.028052 | best val: 0.020197\n",
      "  Epoch  22/80 | train MSE: 0.038359 | val MSE: 0.021761 | best val: 0.020197\n",
      "  Epoch  23/80 | train MSE: 0.037637 | val MSE: 0.024603 | best val: 0.020197\n",
      "  Epoch  24/80 | train MSE: 0.036006 | val MSE: 0.018063 | best val: 0.018063\n",
      "  Epoch  25/80 | train MSE: 0.035649 | val MSE: 0.020366 | best val: 0.018063\n",
      "  Epoch  26/80 | train MSE: 0.035386 | val MSE: 0.020499 | best val: 0.018063\n",
      "  Epoch  27/80 | train MSE: 0.035163 | val MSE: 0.018483 | best val: 0.018063\n",
      "  Epoch  28/80 | train MSE: 0.034770 | val MSE: 0.018089 | best val: 0.018063\n",
      "  Epoch  29/80 | train MSE: 0.034604 | val MSE: 0.016728 | best val: 0.016728\n",
      "  Epoch  30/80 | train MSE: 0.034510 | val MSE: 0.018297 | best val: 0.016728\n",
      "  Epoch  31/80 | train MSE: 0.034432 | val MSE: 0.018097 | best val: 0.016728\n",
      "  Epoch  32/80 | train MSE: 0.033702 | val MSE: 0.017439 | best val: 0.016728\n",
      "  Epoch  33/80 | train MSE: 0.033348 | val MSE: 0.013602 | best val: 0.013602\n",
      "  Epoch  34/80 | train MSE: 0.032916 | val MSE: 0.019273 | best val: 0.013602\n",
      "  Epoch  35/80 | train MSE: 0.032992 | val MSE: 0.016278 | best val: 0.013602\n",
      "  Epoch  36/80 | train MSE: 0.032745 | val MSE: 0.017591 | best val: 0.013602\n",
      "  Epoch  37/80 | train MSE: 0.032309 | val MSE: 0.018017 | best val: 0.013602\n",
      "  Epoch  38/80 | train MSE: 0.031844 | val MSE: 0.020969 | best val: 0.013602\n",
      "  Epoch  39/80 | train MSE: 0.031381 | val MSE: 0.018481 | best val: 0.013602\n",
      "  Epoch  40/80 | train MSE: 0.031395 | val MSE: 0.015331 | best val: 0.013602\n",
      "  Epoch  41/80 | train MSE: 0.030932 | val MSE: 0.018771 | best val: 0.013602\n",
      "  Epoch  42/80 | train MSE: 0.030330 | val MSE: 0.017055 | best val: 0.013602\n",
      "  Epoch  43/80 | train MSE: 0.029861 | val MSE: 0.015233 | best val: 0.013602\n",
      "  Epoch  44/80 | train MSE: 0.029705 | val MSE: 0.014458 | best val: 0.013602\n",
      "  Epoch  45/80 | train MSE: 0.029371 | val MSE: 0.014046 | best val: 0.013602\n",
      "  Epoch  46/80 | train MSE: 0.028814 | val MSE: 0.012424 | best val: 0.012424\n",
      "  Epoch  47/80 | train MSE: 0.028660 | val MSE: 0.016023 | best val: 0.012424\n",
      "  Epoch  48/80 | train MSE: 0.028518 | val MSE: 0.014132 | best val: 0.012424\n",
      "  Epoch  49/80 | train MSE: 0.028169 | val MSE: 0.014131 | best val: 0.012424\n",
      "  Epoch  50/80 | train MSE: 0.028125 | val MSE: 0.013338 | best val: 0.012424\n",
      "  Epoch  51/80 | train MSE: 0.027697 | val MSE: 0.014736 | best val: 0.012424\n",
      "  Epoch  52/80 | train MSE: 0.027481 | val MSE: 0.013308 | best val: 0.012424\n",
      "  Epoch  53/80 | train MSE: 0.027803 | val MSE: 0.012978 | best val: 0.012424\n",
      "  Epoch  54/80 | train MSE: 0.027318 | val MSE: 0.013262 | best val: 0.012424\n",
      "  Epoch  55/80 | train MSE: 0.027362 | val MSE: 0.013603 | best val: 0.012424\n",
      "  Epoch  56/80 | train MSE: 0.026836 | val MSE: 0.012537 | best val: 0.012424\n",
      "  Epoch  57/80 | train MSE: 0.027141 | val MSE: 0.014049 | best val: 0.012424\n",
      "  Epoch  58/80 | train MSE: 0.026361 | val MSE: 0.013202 | best val: 0.012424\n",
      "  Epoch  59/80 | train MSE: 0.026238 | val MSE: 0.013749 | best val: 0.012424\n",
      "  Epoch  60/80 | train MSE: 0.026514 | val MSE: 0.013227 | best val: 0.012424\n",
      "  Epoch  61/80 | train MSE: 0.026532 | val MSE: 0.012373 | best val: 0.012373\n",
      "  Epoch  62/80 | train MSE: 0.026125 | val MSE: 0.012457 | best val: 0.012373\n",
      "  Epoch  63/80 | train MSE: 0.026058 | val MSE: 0.013375 | best val: 0.012373\n",
      "  Epoch  64/80 | train MSE: 0.025907 | val MSE: 0.013114 | best val: 0.012373\n",
      "  Epoch  65/80 | train MSE: 0.025688 | val MSE: 0.013620 | best val: 0.012373\n",
      "  Epoch  66/80 | train MSE: 0.025688 | val MSE: 0.012565 | best val: 0.012373\n",
      "  Epoch  67/80 | train MSE: 0.025910 | val MSE: 0.014244 | best val: 0.012373\n",
      "  Epoch  68/80 | train MSE: 0.025515 | val MSE: 0.014485 | best val: 0.012373\n",
      "  Epoch  69/80 | train MSE: 0.025214 | val MSE: 0.013880 | best val: 0.012373\n",
      "  Epoch  70/80 | train MSE: 0.025242 | val MSE: 0.013894 | best val: 0.012373\n",
      "  Epoch  71/80 | train MSE: 0.025232 | val MSE: 0.013923 | best val: 0.012373\n",
      "  Epoch  72/80 | train MSE: 0.025394 | val MSE: 0.014087 | best val: 0.012373\n",
      "  Epoch  73/80 | train MSE: 0.025203 | val MSE: 0.013792 | best val: 0.012373\n",
      "  Epoch  74/80 | train MSE: 0.025142 | val MSE: 0.013012 | best val: 0.012373\n",
      "  Epoch  75/80 | train MSE: 0.024868 | val MSE: 0.013547 | best val: 0.012373\n",
      "  Epoch  76/80 | train MSE: 0.024998 | val MSE: 0.014173 | best val: 0.012373\n",
      "  Epoch  77/80 | train MSE: 0.024876 | val MSE: 0.013582 | best val: 0.012373\n",
      "  Epoch  78/80 | train MSE: 0.024769 | val MSE: 0.014247 | best val: 0.012373\n",
      "  Epoch  79/80 | train MSE: 0.024754 | val MSE: 0.014217 | best val: 0.012373\n",
      "  Epoch  80/80 | train MSE: 0.024704 | val MSE: 0.014270 | best val: 0.012373\n",
      "\n",
      "Restored best model (val MSE: 0.012373)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 7. TRAIN\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Halve LR when val loss stops improving — helps avoid getting stuck\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=8, \n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state    = None\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(xb)\n",
    "\n",
    "    epoch_loss /= len(X_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = loss_fn(model(X_val), Y_val).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "\n",
    "    print(f\"  Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"train MSE: {epoch_loss:.6f} | \"\n",
    "              f\"val MSE: {val_loss:.6f} | \"\n",
    "              f\"best val: {best_val_loss:.6f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nRestored best model (val MSE: {best_val_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c148dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "VALIDATION RESULTS (original volatility scale)\n",
      "=======================================================\n",
      "  Overall RMSE : 0.009964\n",
      "  Overall MAE  : 0.007222\n",
      "  (Volatility range ≈ 0.02 – 0.45)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 8. EVALUATE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred_np = model(X_val).numpy()\n",
    "\n",
    "val_true_np       = Y_val.numpy()\n",
    "val_pred_original = scaler.inverse_transform(val_pred_np)\n",
    "val_true_original = scaler.inverse_transform(val_true_np)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(val_true_original, val_pred_original))\n",
    "mae  = np.mean(np.abs(val_true_original - val_pred_original))\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"VALIDATION RESULTS (original volatility scale)\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Overall RMSE : {rmse:.6f}\")\n",
    "print(f\"  Overall MAE  : {mae:.6f}\")\n",
    "print(f\"  (Volatility range ≈ 0.02 – 0.45)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7d5895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted next-day surface (first 5 values): [0.02436161 0.03614487 0.03986429 0.04309416 0.04257764]\n"
     ]
    }
   ],
   "source": [
    "last_window_raw    = raw_data[-LOOKBACK:]\n",
    "last_window_scaled = scaler.transform(last_window_raw)\n",
    "last_window_pca    = pca.transform(last_window_scaled)\n",
    "last_window_pca    = pca_scaler.transform(last_window_pca).astype(np.float32)\n",
    "last_window_flat   = last_window_pca.flatten()[np.newaxis, :]    # (1, 80)\n",
    "last_day_tensor    = torch.tensor(last_window_flat, device=DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    next_day_scaled = model(last_day_tensor).numpy()\n",
    "\n",
    "next_day_pred = scaler.inverse_transform(next_day_scaled)\n",
    "print(f\"\\nPredicted next-day surface (first 5 values): {next_day_pred[0, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682fc378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "\n",
      "Model saved → qrc_swaption_model.pt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 10. SAVE\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"test\")\n",
    "torch.save({\n",
    "    \"model_state\"  : model.state_dict(),\n",
    "    \"pca\"          : pca,\n",
    "    \"pca_scaler\"   : pca_scaler,\n",
    "    \"scaler\"       : scaler,\n",
    "    \"feature_cols\" : feature_cols,\n",
    "    \"config\": {\n",
    "        \"N_PCA_COMPONENTS\"  : N_PCA_COMPONENTS,\n",
    "        \"LOOKBACK\"          : LOOKBACK,\n",
    "        \"N_MODES\"           : N_MODES,\n",
    "        \"N_PHOTONS\"         : N_PHOTONS,\n",
    "        \"N_GROUPED_OUTPUTS\" : N_GROUPED_OUTPUTS,\n",
    "    }\n",
    "}, \"qrc_swaption_model.pt\")\n",
    "\n",
    "print(\"\\nModel saved → qrc_swaption_model.pt\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
