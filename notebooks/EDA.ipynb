{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a204f",
   "metadata": {},
   "source": [
    "# Level 1 vs Level 2 data comparision\n",
    "TLDR: L2 is L1 with 5 rows missing. So the task of predictions from L1 still remains important. Once this is working, then adding data imputation to L2 will complete the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets library from hugging face\n",
    "\n",
    "# Level 1 data\n",
    "ds_level1 = load_dataset(\n",
    "\"Quandela/Challenge_Swaptions\",\n",
    "data_files=\"level-1_Future_prediction/train.csv\",\n",
    "split=\"train\",\n",
    "download_mode=\"force_redownload\")\n",
    "\n",
    "df = ds_level1.to_pandas()\n",
    "# Convert price columns from string to float\n",
    "price_cols_all = [c for c in df.columns if c != 'Date']\n",
    "df[price_cols_all] = df[price_cols_all].astype(float)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd07130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic shape and info\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_level2 = load_dataset(\n",
    "    \"Quandela/Challenge_Swaptions\",\n",
    "    data_files=\"level-2_Missing_data_prediction/train_level2.csv\",\n",
    "    split=\"train\",\n",
    ")\n",
    "df2 = ds_level2.to_pandas()\n",
    "# Convert price columns from string to float\n",
    "price_cols_all = [c for c in df2.columns if c != 'Date']\n",
    "df2[price_cols_all] = df2[price_cols_all].astype(float)\n",
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic shape and info\n",
    "print(f\"Shape: {df2.shape}\")\n",
    "df2.info()\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98343cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing rows (in L1 but not L2) by Date\n",
    "missing = df[~df['Date'].isin(df2['Date'])]\n",
    "print(f\"Missing {len(missing)} rows from L2:\\n\")\n",
    "print(missing['Date'].tolist())\n",
    "\n",
    "# Check if shared rows have identical values\n",
    "merged = df.merge(df2, on='Date', suffixes=('_L1', '_L2'))\n",
    "diffs = 0\n",
    "for col in price_cols_all:\n",
    "    mask = merged[f'{col}_L1'] != merged[f'{col}_L2']\n",
    "    diffs += mask.sum()\n",
    "print(f\"\\nValue differences across shared rows: {diffs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ad15a",
   "metadata": {},
   "source": [
    "L1 has 494 records and L2 has 489. Only 5 records are missing. The rest of the data is identical. The five missing dates are: ` ['01/03/2050', '14/04/2050', '28/05/2050', '17/08/2050', '20/10/2050']` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829e0f4",
   "metadata": {},
   "source": [
    "# EDA L1\n",
    "## Data Structure\n",
    "\n",
    "- **494 rows** = trading days (~2 years daily data)\n",
    "- **224 price columns** + 1 `Date` column (wide format — each row is one day's full pricing surface)\n",
    "\n",
    "### The Grid: 14 Tenors × 16 Maturities = 224 instruments\n",
    "\n",
    "| Dimension | Values (years) |\n",
    "|---|---|\n",
    "| **Tenor** (how long the swap lasts) | 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30 |\n",
    "| **Maturity** (when the option expires) | 0.083, 0.25, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5, 7, 10, 15, 20, 25, 30 |\n",
    "\n",
    "E.g. `Tenor : 5; Maturity : 2` = price of an option expiring in 2 years to enter a 5-year swap.\n",
    "\n",
    "**Task:** Given a sequence of 224-d daily surface snapshots, predict the next ~10 trading days (2 weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac91573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and check frequency\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed')\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Total days: {df.shape[0]}\")\n",
    "print(f\"Gaps > 3 calendar days: {(df['Date'].diff().dt.days > 3).sum()}\")\n",
    "print(f\"Null values: {df.iloc[:, 1:].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647531c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tenor/maturity from column names\n",
    "price_cols = [c for c in df.columns if c != 'Date']\n",
    "tenors, maturities = [], []\n",
    "for c in price_cols:\n",
    "    parts = c.split(';')\n",
    "    t = float(parts[0].split(':')[1].strip())\n",
    "    m = float(parts[1].split(':')[1].strip())\n",
    "    tenors.append(t)\n",
    "    maturities.append(m)\n",
    "\n",
    "unique_tenors = sorted(set(tenors))\n",
    "unique_maturities = sorted(set(maturities))\n",
    "print(f\"Tenors ({len(unique_tenors)}): {unique_tenors}\")\n",
    "print(f\"Maturities ({len(unique_maturities)}): {unique_maturities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: swaption surface for a single day\n",
    "day_idx = 0  # first day\n",
    "surface = df.iloc[day_idx, 1:].values.astype(float).reshape(len(unique_maturities), len(unique_tenors))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "im = ax.imshow(surface, aspect='auto', cmap='viridis', origin='lower')\n",
    "ax.set_xticks(range(len(unique_tenors)))\n",
    "ax.set_xticklabels([f\"{t:.0f}\" if t >= 1 else f\"{t}\" for t in unique_tenors], rotation=45)\n",
    "ax.set_yticks(range(len(unique_maturities)))\n",
    "ax.set_yticklabels([f\"{m:.2f}\" if m < 1 else f\"{m:.0f}\" for m in unique_maturities])\n",
    "ax.set_xlabel('Tenor (years)')\n",
    "ax.set_ylabel('Maturity (years)')\n",
    "ax.set_title(f\"Swaption Surface — {df['Date'].iloc[day_idx]}\")\n",
    "plt.colorbar(im, label='Price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series of a few representative (tenor, maturity) pairs\n",
    "samples = ['Tenor : 5; Maturity : 2', 'Tenor : 10; Maturity : 5', \n",
    "           'Tenor : 1; Maturity : 0.25', 'Tenor : 30; Maturity : 30']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for col in samples:\n",
    "    ax.plot(df['Date'], df[col], label=col, alpha=0.8)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_title('Swaption Price Time Series (selected instruments)')\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f311f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation structure — how related are the 224 instruments?\n",
    "corr = df[price_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr.values, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix (224 instruments)')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean pairwise correlation: {corr.values[np.triu_indices(224, k=1)].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c960dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA — how many components capture the surface variance?\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(df[price_cols].values)\n",
    "pca = PCA().fit(X)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, len(cumvar)+1), cumvar, 'b-')\n",
    "ax.axhline(0.95, color='r', linestyle='--', label='95% variance')\n",
    "ax.axhline(0.99, color='orange', linestyle='--', label='99% variance')\n",
    "n95 = np.argmax(cumvar >= 0.95) + 1\n",
    "n99 = np.argmax(cumvar >= 0.99) + 1\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Explained Variance')\n",
    "ax.set_title('PCA on Swaption Surface')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Components for 95% variance: {n95}\")\n",
    "print(f\"Components for 99% variance: {n99}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b9b11",
   "metadata": {},
   "source": [
    "## PCA Result\n",
    "\n",
    "**3 components capture 99% of the variance** — the 224-dimensional surface can be described by just 3 numbers per day.\n",
    "\n",
    "- **PC1** = overall level (all prices shift up/down together)\n",
    "- **PC2** = slope (short vs long end move differently)\n",
    "- **PC3** = curvature (middle vs extremes)\n",
    "\n",
    "**Implication:** We compress 224 → 3 features via PCA, then predict a time-series of 3-d vectors. This fits easily within the 20-mode quantum circuit limit. After prediction, inverse-PCA reconstructs the full 224-price surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03218ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of daily price changes (returns)\n",
    "returns = df[price_cols].diff().iloc[1:]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(returns.values.flatten(), bins=100, edgecolor='none', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Daily Price Changes')\n",
    "axes[0].set_xlabel('ΔPrice')\n",
    "\n",
    "# Volatility across the surface (std of daily changes per instrument)\n",
    "vol = returns.std()\n",
    "vol_surface = vol.values.reshape(len(unique_maturities), len(unique_tenors))\n",
    "im = axes[1].imshow(vol_surface, aspect='auto', cmap='hot', origin='lower')\n",
    "axes[1].set_xticks(range(len(unique_tenors)))\n",
    "axes[1].set_xticklabels([f\"{t:.0f}\" if t >= 1 else f\"{t}\" for t in unique_tenors], rotation=45)\n",
    "axes[1].set_yticks(range(len(unique_maturities)))\n",
    "axes[1].set_yticklabels([f\"{m:.2f}\" if m < 1 else f\"{m:.0f}\" for m in unique_maturities])\n",
    "axes[1].set_xlabel('Tenor')\n",
    "axes[1].set_ylabel('Maturity')\n",
    "axes[1].set_title('Volatility of Daily Changes')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114783d2",
   "metadata": {},
   "source": [
    "## Volatility Surface Observations\n",
    "\n",
    "**Important:** The dataset values are **implied volatilities**, not raw prices. Predicting these IV values is the challenge objective.\n",
    "\n",
    "Daily change volatility observations:\n",
    "- **Lowest volatility:** 1-month maturity row (bottom) — short-dated options are most stable day-to-day\n",
    "- **Highest volatility (~0.025):** tenors <7 with maturities 2–7 — the \"belly\" of the surface moves most\n",
    "- **Long maturities (20+):** also elevated — far-future options are harder to predict\n",
    "- **Overall range is narrow** — daily moves are small, so the model needs to be precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84860885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation — how predictable is tomorrow from today?\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Check autocorrelation on the first 3 PCA components\n",
    "X_pca = pca.transform(X)[:, :3]\n",
    "pc_labels = ['PC1 (level)', 'PC2 (slope)', 'PC3 (curvature)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "for i, (ax, label) in enumerate(zip(axes, pc_labels)):\n",
    "    ac = acf(X_pca[:, i], nlags=20)\n",
    "    ax.bar(range(len(ac)), ac, width=0.5)\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel('Lag (days)')\n",
    "    ax.set_ylabel('Autocorrelation')\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "plt.suptitle('Autocorrelation of PCA Components', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d813a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationarity check — are PCA components trending or mean-reverting?\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for i, label in enumerate(pc_labels):\n",
    "    result = adfuller(X_pca[:, i])\n",
    "    status = \"Stationary\" if result[1] < 0.05 else \"Non-stationary (trending)\"\n",
    "    print(f\"{label}: ADF p-value={result[1]:.4f} → {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28596364",
   "metadata": {},
   "source": [
    "## EDA Summary\n",
    "\n",
    "| Finding | Implication |\n",
    "|---|---|\n",
    "| 224 IV values compress to **3 PCA components** (99% variance) | Model input is just 3 features per day |\n",
    "| High autocorrelation → tomorrow looks like today | Time-series forecasting is feasible |\n",
    "| If non-stationary → components trend over time | May need to predict **daily changes** (Δ) instead of levels |\n",
    "| Narrow daily change range (~0.025 max) | Model must be precise; small errors matter |\n",
    "| Values are **implied volatilities** | Predict IV directly — this is the challenge objective |\n",
    "\n",
    "**Next step:** Build a classical baseline (MLP/RNN) on PCA-compressed IV time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
