{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d262018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nQML Model — Level 2: Swaption Surface Missing Data Reconstruction\\nArchitecture: Photonic Quantum Circuit with MerLin CircuitBuilder\\n\\nProblem:\\n    The test set will have certain maturity rows hidden (from our EDA: 1yr and 1.5yr).\\n    The training data is fully observed — so we artificially mask those rows during\\n    training to teach the model to reconstruct them from the remaining surface.\\n\\nPipeline:\\n    1. Load Level 2 training data (fully observed)\\n    2. Separate into:\\n         - Input  X: all columns EXCEPT the target maturities (196 values)\\n         - Target Y: the target maturity columns (28 values = 14 tenors × 2 maturities)\\n    3. Preprocess: MinMaxScaler → PCA (196 → 16) → MinMaxScaler\\n    4. Quantum circuit (CircuitBuilder, 16 modes, 3 photons):\\n         - entangling layer (trainable)\\n         - angle encoding (scale=π)\\n         - rotations (trainable)\\n         - superpositions (trainable)\\n    5. MeasurementStrategy.mode_expectations() → 16 compact features\\n       (one per mode — ideal for small regression targets, no LexGrouping needed)\\n    6. Classical readout → 28 reconstructed volatilities\\n\\nKey difference from Level 1:\\n    - No LOOKBACK / time series component — purely spatial interpolation\\n    - Smaller output (28 vs 224 values)\\n    - mode_expectations() instead of probs() — more compact, faster\\n    - Input is a subset of the surface (not a time window)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "QML Model — Level 2: Swaption Surface Missing Data Reconstruction\n",
    "Architecture: Photonic Quantum Circuit with MerLin CircuitBuilder\n",
    "\n",
    "Problem:\n",
    "    The test set will have certain maturity rows hidden (from our EDA: 1yr and 1.5yr).\n",
    "    The training data is fully observed — so we artificially mask those rows during\n",
    "    training to teach the model to reconstruct them from the remaining surface.\n",
    "\n",
    "Pipeline:\n",
    "    1. Load Level 2 training data (fully observed)\n",
    "    2. Separate into:\n",
    "         - Input  X: all columns EXCEPT the target maturities (196 values)\n",
    "         - Target Y: the target maturity columns (28 values = 14 tenors × 2 maturities)\n",
    "    3. Preprocess: MinMaxScaler → PCA (196 → 16) → MinMaxScaler\n",
    "    4. Quantum circuit (CircuitBuilder, 16 modes, 3 photons):\n",
    "         - entangling layer (trainable)\n",
    "         - angle encoding (scale=π)\n",
    "         - rotations (trainable)\n",
    "         - superpositions (trainable)\n",
    "    5. MeasurementStrategy.mode_expectations() → 16 compact features\n",
    "       (one per mode — ideal for small regression targets, no LexGrouping needed)\n",
    "    6. Classical readout → 28 reconstructed volatilities\n",
    "\n",
    "Key difference from Level 1:\n",
    "    - No LOOKBACK / time series component — purely spatial interpolation\n",
    "    - Smaller output (28 vs 224 values)\n",
    "    - mode_expectations() instead of probs() — more compact, faster\n",
    "    - Input is a subset of the surface (not a time window)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40849e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datasets import load_dataset\n",
    "\n",
    "import merlin as ML\n",
    "from merlin import MeasurementStrategy\n",
    "from merlin.builder import CircuitBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7804e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# Target maturities to reconstruct (from EDA: these are the hidden ones)\n",
    "TARGET_MATURITIES = [1.0, 1.5]\n",
    "\n",
    "N_PCA_COMPONENTS  = 16    # PCA compress observed features → quantum-compatible size\n",
    "N_MODES           = 16    # Quantum circuit modes (≤ 20 QPU hard limit)\n",
    "N_PHOTONS         = 3     # Fewer photons than Level 1: smaller task, faster simulation\n",
    "TRAIN_SPLIT       = 0.85\n",
    "EPOCHS            = 100   # More epochs: smaller dataset (489 rows) and simpler task\n",
    "LR                = 5e-4\n",
    "BATCH_SIZE        = 16\n",
    "DEVICE            = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7daff477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Level 2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 224\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 1. LOAD DATA\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "print(\"Loading Level 2 dataset...\")\n",
    "ds = load_dataset(\n",
    "    \"Quandela/Challenge_Swaptions\",\n",
    "    data_files=\"level-2_Missing_data_prediction/train_level2.csv\",\n",
    "    split=\"train\",\n",
    ")\n",
    "df = ds.to_pandas()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "all_feat_cols = [c for c in df.columns if c != \"Date\"]\n",
    "print(f\"Total columns: {len(all_feat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614cbffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input columns  (observed surface): 196\n",
      "Target columns (to reconstruct)  : 28\n",
      "Target maturities: [1.0, 1.5]\n",
      "Tenors covered   : 14 (28 total target cells)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2. SPLIT COLUMNS: OBSERVED vs TARGET\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# Target columns = maturities we need to reconstruct (1yr and 1.5yr)\n",
    "# Input columns  = everything else (used to predict the targets)\n",
    "\n",
    "target_cols = [\n",
    "    c for c in all_feat_cols\n",
    "    if float(c.split(\"Maturity : \")[1]) in TARGET_MATURITIES\n",
    "]\n",
    "input_cols = [c for c in all_feat_cols if c not in target_cols]\n",
    "\n",
    "print(f\"\\nInput columns  (observed surface): {len(input_cols)}\")\n",
    "print(f\"Target columns (to reconstruct)  : {len(target_cols)}\")\n",
    "print(f\"Target maturities: {TARGET_MATURITIES}\")\n",
    "print(f\"Tenors covered   : {len(target_cols) // len(TARGET_MATURITIES)} \"\n",
    "      f\"({len(target_cols)} total target cells)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ba1eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA explained variance: 100.0%\n",
      "X after PCA: (489, 16)\n",
      "Y shape    : (489, 28)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 3. PREPROCESS\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "X_raw = df[input_cols].values.astype(np.float32)   # (489, 196)\n",
    "Y_raw = df[target_cols].values.astype(np.float32)  # (489, 28)\n",
    "\n",
    "# Scale inputs to [0,1]\n",
    "x_scaler = MinMaxScaler()\n",
    "X_scaled  = x_scaler.fit_transform(X_raw).astype(np.float32)\n",
    "\n",
    "# Scale targets to [0,1] — stored separately for inverse transform at eval time\n",
    "y_scaler = StandardScaler ()\n",
    "Y_scaled  = y_scaler.fit_transform(Y_raw).astype(np.float32)\n",
    "\n",
    "# PCA: 196 → 16 (to fit quantum mode limit)\n",
    "pca = PCA(n_components=N_PCA_COMPONENTS)\n",
    "X_pca = pca.fit_transform(X_scaled).astype(np.float32)\n",
    "\n",
    "# Re-scale PCA outputs to [0,1] for stable angle encoding\n",
    "pca_scaler = MinMaxScaler()\n",
    "X_pca = pca_scaler.fit_transform(X_pca).astype(np.float32)\n",
    "\n",
    "print(f\"\\nPCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"X after PCA: {X_pca.shape}\")\n",
    "print(f\"Y shape    : {Y_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fed23514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 415 samples | Val: 74 samples\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 4. TRAIN / VAL SPLIT  (chronological)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "split   = int(len(X_pca) * TRAIN_SPLIT)\n",
    "X_train = torch.tensor(X_pca[:split],    device=DEVICE)\n",
    "Y_train = torch.tensor(Y_scaled[:split], device=DEVICE)\n",
    "X_val   = torch.tensor(X_pca[split:],    device=DEVICE)\n",
    "Y_val   = torch.tensor(Y_scaled[split:], device=DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} samples | Val: {len(X_val)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a20a3a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum layer output size (mode expectations): 16\n"
     ]
    }
   ],
   "source": [
    "builder = CircuitBuilder(n_modes=N_MODES)\n",
    "builder.add_entangling_layer(trainable=True, name=\"U1\")\n",
    "builder.add_angle_encoding(\n",
    "    modes=list(range(N_MODES)),\n",
    "    name=\"input\",\n",
    "    scale=np.pi,\n",
    ")\n",
    "builder.add_rotations(trainable=True, name=\"theta\")\n",
    "builder.add_superpositions(depth=2, trainable=True)   # depth=2 for richer interference\n",
    "\n",
    "quantum_core = ML.QuantumLayer(\n",
    "    input_size=N_MODES,\n",
    "    builder=builder,\n",
    "    n_photons=N_PHOTONS,\n",
    "    measurement_strategy=MeasurementStrategy.mode_expectations(),\n",
    "    # mode_expectations: output_size = n_modes = 16\n",
    "    # One compact value per mode — perfect bridge to our 28-output readout\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantum layer output size (mode expectations): {quantum_core.output_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67798304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total params     : 12,952\n",
      "Trainable params : 12,952\n"
     ]
    }
   ],
   "source": [
    "class QRCMissingData(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Quantum feature extraction — no pre-compression needed here\n",
    "        # PCA already outputs exactly N_MODES=16 features\n",
    "        self.quantum = quantum_core\n",
    "\n",
    "        # Classical readout: 16 quantum features → 28 missing values\n",
    "        # Smaller network than Level 1 (task is simpler: 28 outputs vs 224)\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(quantum_core.output_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quantum(x)    # (B, 16) → (B, 16) mode expectations\n",
    "        return self.readout(x) # (B, 16) → (B, 28)\n",
    "\n",
    "\n",
    "model = QRCMissingData(output_size=len(target_cols)).to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal params     : {total_params:,}\")\n",
    "print(f\"Trainable params : {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7814dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "  Epoch   1/100 | train MSE: 1.287937 | val MSE: 0.919370 | best val: 0.919370\n",
      "  Epoch   2/100 | train MSE: 1.177521 | val MSE: 0.952854 | best val: 0.919370\n",
      "  Epoch   3/100 | train MSE: 1.131689 | val MSE: 1.024751 | best val: 0.919370\n",
      "  Epoch   4/100 | train MSE: 1.115083 | val MSE: 1.074739 | best val: 0.919370\n",
      "  Epoch   5/100 | train MSE: 1.102538 | val MSE: 1.111048 | best val: 0.919370\n",
      "  Epoch   6/100 | train MSE: 1.101274 | val MSE: 1.120526 | best val: 0.919370\n",
      "  Epoch   7/100 | train MSE: 1.076132 | val MSE: 1.131563 | best val: 0.919370\n",
      "  Epoch   8/100 | train MSE: 1.078146 | val MSE: 1.156233 | best val: 0.919370\n",
      "  Epoch   9/100 | train MSE: 1.073286 | val MSE: 1.147725 | best val: 0.919370\n",
      "  Epoch  10/100 | train MSE: 1.061164 | val MSE: 1.155216 | best val: 0.919370\n",
      "  Epoch  11/100 | train MSE: 1.055896 | val MSE: 1.157140 | best val: 0.919370\n",
      "  Epoch  12/100 | train MSE: 1.050262 | val MSE: 1.161992 | best val: 0.919370\n",
      "  Epoch  13/100 | train MSE: 1.048559 | val MSE: 1.156979 | best val: 0.919370\n",
      "  Epoch  14/100 | train MSE: 1.041143 | val MSE: 1.163767 | best val: 0.919370\n",
      "  Epoch  15/100 | train MSE: 1.035458 | val MSE: 1.154736 | best val: 0.919370\n",
      "  Epoch  16/100 | train MSE: 1.032894 | val MSE: 1.153588 | best val: 0.919370\n",
      "  Epoch  17/100 | train MSE: 1.035278 | val MSE: 1.151644 | best val: 0.919370\n",
      "  Epoch  18/100 | train MSE: 1.037235 | val MSE: 1.148515 | best val: 0.919370\n",
      "  Epoch  19/100 | train MSE: 1.036846 | val MSE: 1.137335 | best val: 0.919370\n",
      "  Epoch  20/100 | train MSE: 1.035815 | val MSE: 1.142180 | best val: 0.919370\n",
      "  Epoch  21/100 | train MSE: 1.029401 | val MSE: 1.146764 | best val: 0.919370\n",
      "  Epoch  22/100 | train MSE: 1.032474 | val MSE: 1.152032 | best val: 0.919370\n",
      "  Epoch  23/100 | train MSE: 1.022941 | val MSE: 1.126969 | best val: 0.919370\n",
      "  Epoch  24/100 | train MSE: 1.017170 | val MSE: 1.130961 | best val: 0.919370\n",
      "  Epoch  25/100 | train MSE: 1.021272 | val MSE: 1.127721 | best val: 0.919370\n",
      "  Epoch  26/100 | train MSE: 1.020087 | val MSE: 1.126776 | best val: 0.919370\n",
      "  Epoch  27/100 | train MSE: 1.014949 | val MSE: 1.135079 | best val: 0.919370\n",
      "  Epoch  28/100 | train MSE: 1.011452 | val MSE: 1.135528 | best val: 0.919370\n",
      "  Epoch  29/100 | train MSE: 1.015137 | val MSE: 1.133925 | best val: 0.919370\n",
      "  Epoch  30/100 | train MSE: 1.013735 | val MSE: 1.134989 | best val: 0.919370\n",
      "  Epoch  31/100 | train MSE: 1.004133 | val MSE: 1.131316 | best val: 0.919370\n",
      "  Epoch  32/100 | train MSE: 1.010211 | val MSE: 1.127580 | best val: 0.919370\n",
      "  Epoch  33/100 | train MSE: 1.008866 | val MSE: 1.132486 | best val: 0.919370\n",
      "  Epoch  34/100 | train MSE: 1.011617 | val MSE: 1.122337 | best val: 0.919370\n",
      "  Epoch  35/100 | train MSE: 1.013023 | val MSE: 1.124350 | best val: 0.919370\n",
      "  Epoch  36/100 | train MSE: 1.016882 | val MSE: 1.123497 | best val: 0.919370\n",
      "  Epoch  37/100 | train MSE: 0.999141 | val MSE: 1.127000 | best val: 0.919370\n",
      "  Epoch  38/100 | train MSE: 1.004087 | val MSE: 1.127205 | best val: 0.919370\n",
      "  Epoch  39/100 | train MSE: 0.999650 | val MSE: 1.131078 | best val: 0.919370\n",
      "  Epoch  40/100 | train MSE: 1.004134 | val MSE: 1.121858 | best val: 0.919370\n",
      "  Epoch  41/100 | train MSE: 1.004940 | val MSE: 1.122270 | best val: 0.919370\n",
      "  Epoch  42/100 | train MSE: 1.008359 | val MSE: 1.118721 | best val: 0.919370\n",
      "  Epoch  43/100 | train MSE: 1.004337 | val MSE: 1.117209 | best val: 0.919370\n",
      "  Epoch  44/100 | train MSE: 0.999932 | val MSE: 1.125698 | best val: 0.919370\n",
      "  Epoch  45/100 | train MSE: 1.012661 | val MSE: 1.117319 | best val: 0.919370\n",
      "  Epoch  46/100 | train MSE: 0.997051 | val MSE: 1.122174 | best val: 0.919370\n",
      "  Epoch  47/100 | train MSE: 0.998541 | val MSE: 1.119185 | best val: 0.919370\n",
      "  Epoch  48/100 | train MSE: 1.008649 | val MSE: 1.122596 | best val: 0.919370\n",
      "  Epoch  49/100 | train MSE: 1.001663 | val MSE: 1.122291 | best val: 0.919370\n",
      "  Epoch  50/100 | train MSE: 1.003210 | val MSE: 1.109116 | best val: 0.919370\n",
      "  Epoch  51/100 | train MSE: 1.004958 | val MSE: 1.120351 | best val: 0.919370\n",
      "  Epoch  52/100 | train MSE: 1.004329 | val MSE: 1.118086 | best val: 0.919370\n",
      "  Epoch  53/100 | train MSE: 1.003282 | val MSE: 1.113115 | best val: 0.919370\n",
      "  Epoch  54/100 | train MSE: 1.000765 | val MSE: 1.114962 | best val: 0.919370\n",
      "  Epoch  55/100 | train MSE: 0.995857 | val MSE: 1.114086 | best val: 0.919370\n",
      "  Epoch  56/100 | train MSE: 0.999437 | val MSE: 1.112411 | best val: 0.919370\n",
      "  Epoch  57/100 | train MSE: 0.995693 | val MSE: 1.114675 | best val: 0.919370\n",
      "  Epoch  58/100 | train MSE: 0.992939 | val MSE: 1.112839 | best val: 0.919370\n",
      "  Epoch  59/100 | train MSE: 1.001157 | val MSE: 1.109767 | best val: 0.919370\n",
      "  Epoch  60/100 | train MSE: 0.998056 | val MSE: 1.110566 | best val: 0.919370\n",
      "  Epoch  61/100 | train MSE: 1.003766 | val MSE: 1.113339 | best val: 0.919370\n",
      "  Epoch  62/100 | train MSE: 0.994272 | val MSE: 1.113000 | best val: 0.919370\n",
      "  Epoch  63/100 | train MSE: 1.000819 | val MSE: 1.106238 | best val: 0.919370\n",
      "  Epoch  64/100 | train MSE: 0.987940 | val MSE: 1.107949 | best val: 0.919370\n",
      "  Epoch  65/100 | train MSE: 1.001117 | val MSE: 1.118059 | best val: 0.919370\n",
      "  Epoch  66/100 | train MSE: 0.984740 | val MSE: 1.121129 | best val: 0.919370\n",
      "  Epoch  67/100 | train MSE: 0.994301 | val MSE: 1.109850 | best val: 0.919370\n",
      "  Epoch  68/100 | train MSE: 0.999094 | val MSE: 1.109246 | best val: 0.919370\n",
      "  Epoch  69/100 | train MSE: 0.997215 | val MSE: 1.113179 | best val: 0.919370\n",
      "  Epoch  70/100 | train MSE: 0.997378 | val MSE: 1.112568 | best val: 0.919370\n",
      "  Epoch  71/100 | train MSE: 0.992640 | val MSE: 1.118503 | best val: 0.919370\n",
      "  Epoch  72/100 | train MSE: 1.003639 | val MSE: 1.114301 | best val: 0.919370\n",
      "  Epoch  73/100 | train MSE: 1.003605 | val MSE: 1.116896 | best val: 0.919370\n",
      "  Epoch  74/100 | train MSE: 0.995477 | val MSE: 1.113933 | best val: 0.919370\n",
      "  Epoch  75/100 | train MSE: 0.989470 | val MSE: 1.102772 | best val: 0.919370\n",
      "  Epoch  76/100 | train MSE: 0.995499 | val MSE: 1.111141 | best val: 0.919370\n",
      "  Epoch  77/100 | train MSE: 0.992591 | val MSE: 1.114916 | best val: 0.919370\n",
      "  Epoch  78/100 | train MSE: 0.998122 | val MSE: 1.112736 | best val: 0.919370\n",
      "  Epoch  79/100 | train MSE: 0.993301 | val MSE: 1.118204 | best val: 0.919370\n",
      "  Epoch  80/100 | train MSE: 0.999146 | val MSE: 1.113320 | best val: 0.919370\n",
      "  Epoch  81/100 | train MSE: 0.996634 | val MSE: 1.105197 | best val: 0.919370\n",
      "  Epoch  82/100 | train MSE: 0.992322 | val MSE: 1.110858 | best val: 0.919370\n",
      "  Epoch  83/100 | train MSE: 0.998209 | val MSE: 1.115038 | best val: 0.919370\n",
      "  Epoch  84/100 | train MSE: 0.996488 | val MSE: 1.106339 | best val: 0.919370\n",
      "  Epoch  85/100 | train MSE: 0.997798 | val MSE: 1.110042 | best val: 0.919370\n",
      "  Epoch  86/100 | train MSE: 0.994872 | val MSE: 1.115359 | best val: 0.919370\n",
      "  Epoch  87/100 | train MSE: 0.995339 | val MSE: 1.113472 | best val: 0.919370\n",
      "  Epoch  88/100 | train MSE: 0.993003 | val MSE: 1.113995 | best val: 0.919370\n",
      "  Epoch  89/100 | train MSE: 0.995922 | val MSE: 1.108204 | best val: 0.919370\n",
      "  Epoch  90/100 | train MSE: 0.993370 | val MSE: 1.116622 | best val: 0.919370\n",
      "  Epoch  91/100 | train MSE: 1.004696 | val MSE: 1.110736 | best val: 0.919370\n",
      "  Epoch  92/100 | train MSE: 0.994498 | val MSE: 1.114593 | best val: 0.919370\n",
      "  Epoch  93/100 | train MSE: 0.994936 | val MSE: 1.113255 | best val: 0.919370\n",
      "  Epoch  94/100 | train MSE: 1.001709 | val MSE: 1.107569 | best val: 0.919370\n",
      "  Epoch  95/100 | train MSE: 0.987433 | val MSE: 1.111067 | best val: 0.919370\n",
      "  Epoch  96/100 | train MSE: 0.992782 | val MSE: 1.101479 | best val: 0.919370\n",
      "  Epoch  97/100 | train MSE: 0.998171 | val MSE: 1.099383 | best val: 0.919370\n",
      "  Epoch  98/100 | train MSE: 0.994780 | val MSE: 1.099858 | best val: 0.919370\n",
      "  Epoch  99/100 | train MSE: 0.996902 | val MSE: 1.108346 | best val: 0.919370\n",
      "  Epoch 100/100 | train MSE: 0.998034 | val MSE: 1.107054 | best val: 0.919370\n",
      "\n",
      "Restored best model (val MSE: 0.919370)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10, \n",
    ")\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state    = None\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(xb)\n",
    "\n",
    "    epoch_loss /= len(X_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = loss_fn(model(X_val), Y_val).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    \n",
    "    print(f\"  Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"train MSE: {epoch_loss:.6f} | \"\n",
    "              f\"val MSE: {val_loss:.6f} | \"\n",
    "              f\"best val: {best_val_loss:.6f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nRestored best model (val MSE: {best_val_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9025399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "VALIDATION RESULTS (original volatility scale)\n",
      "=======================================================\n",
      "  Overall RMSE : 0.017870\n",
      "  Overall MAE  : 0.013245\n",
      "  (Volatility range ≈ 0.02 – 0.45)\n",
      "\n",
      "  Per-maturity breakdown:\n",
      "    Maturity 1.0yr → RMSE: 0.016318 | MAE: 0.012519\n",
      "    Maturity 1.5yr → RMSE: 0.019298 | MAE: 0.013971\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 8. EVALUATE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred_np = model(X_val).numpy()\n",
    "\n",
    "val_true_np = Y_val.numpy()\n",
    "\n",
    "# Inverse transform back to original volatility scale\n",
    "val_pred_original = y_scaler.inverse_transform(val_pred_np)\n",
    "val_true_original = y_scaler.inverse_transform(val_true_np)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(val_true_original, val_pred_original))\n",
    "mae  = np.mean(np.abs(val_true_original - val_pred_original))\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"VALIDATION RESULTS (original volatility scale)\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Overall RMSE : {rmse:.6f}\")\n",
    "print(f\"  Overall MAE  : {mae:.6f}\")\n",
    "print(f\"  (Volatility range ≈ 0.02 – 0.45)\")\n",
    "\n",
    "# Per-maturity breakdown\n",
    "print(f\"\\n  Per-maturity breakdown:\")\n",
    "tenors_list = sorted(set(\n",
    "    int(c.split(\"Tenor : \")[1].split(\";\")[0]) for c in target_cols\n",
    "))\n",
    "for mat in TARGET_MATURITIES:\n",
    "    mat_cols_idx = [\n",
    "        i for i, c in enumerate(target_cols)\n",
    "        if float(c.split(\"Maturity : \")[1]) == mat\n",
    "    ]\n",
    "    mat_pred = val_pred_original[:, mat_cols_idx]\n",
    "    mat_true = val_true_original[:, mat_cols_idx]\n",
    "    mat_rmse = np.sqrt(mean_squared_error(mat_true, mat_pred))\n",
    "    mat_mae  = np.mean(np.abs(mat_true - mat_pred))\n",
    "    print(f\"    Maturity {mat}yr → RMSE: {mat_rmse:.6f} | MAE: {mat_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d78122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example reconstruction (last training row):\n",
      "  T1/M1        → predicted: 0.158804 | true: 0.132854\n",
      "  T2/M1        → predicted: 0.170556 | true: 0.135154\n",
      "  T3/M1        → predicted: 0.165560 | true: 0.135435\n",
      "  T4/M1        → predicted: 0.162083 | true: 0.132703\n",
      "  T5/M1        → predicted: 0.169149 | true: 0.132891\n",
      "  T6/M1        → predicted: 0.155651 | true: 0.127393\n",
      "  T7/M1        → predicted: 0.153856 | true: 0.127258\n",
      "  T8/M1        → predicted: 0.146991 | true: 0.124846\n",
      "  T9/M1        → predicted: 0.144825 | true: 0.123276\n",
      "  T10/M1       → predicted: 0.148325 | true: 0.124676\n",
      "  T15/M1       → predicted: 0.144887 | true: 0.116959\n",
      "  T20/M1       → predicted: 0.135959 | true: 0.116219\n",
      "  T25/M1       → predicted: 0.132698 | true: 0.115657\n",
      "  T30/M1       → predicted: 0.134548 | true: 0.115668\n",
      "  T1/M1.5      → predicted: 0.208766 | true: 0.165595\n",
      "  T2/M1.5      → predicted: 0.207043 | true: 0.164118\n",
      "  T3/M1.5      → predicted: 0.204665 | true: 0.161617\n",
      "  T4/M1.5      → predicted: 0.214113 | true: 0.162557\n",
      "  T5/M1.5      → predicted: 0.194213 | true: 0.160889\n",
      "  T6/M1.5      → predicted: 0.188921 | true: 0.157250\n",
      "  T7/M1.5      → predicted: 0.182214 | true: 0.154248\n",
      "  T8/M1.5      → predicted: 0.186238 | true: 0.151388\n",
      "  T9/M1.5      → predicted: 0.177456 | true: 0.149551\n",
      "  T10/M1.5     → predicted: 0.169962 | true: 0.145695\n",
      "  T15/M1.5     → predicted: 0.167911 | true: 0.142318\n",
      "  T20/M1.5     → predicted: 0.150648 | true: 0.135849\n",
      "  T25/M1.5     → predicted: 0.154558 | true: 0.136090\n",
      "  T30/M1.5     → predicted: 0.159114 | true: 0.139135\n",
      "  ... (showing 5 of 28)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 9. RECONSTRUCT A FULL ROW  (ready for test set)\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# At test time: you receive a row with 1yr and 1.5yr columns missing.\n",
    "# Feed the observed columns through the pipeline to get predictions.\n",
    "\n",
    "sample_row_raw    = X_raw[-1:]                                          # (1, 196)\n",
    "sample_row_scaled = x_scaler.transform(sample_row_raw)\n",
    "sample_row_pca    = pca.transform(sample_row_scaled)\n",
    "sample_row_pca    = pca_scaler.transform(sample_row_pca).astype(np.float32)\n",
    "sample_tensor     = torch.tensor(sample_row_pca, device=DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_scaled = model(sample_tensor).numpy()\n",
    "\n",
    "pred_original = y_scaler.inverse_transform(pred_scaled)\n",
    "\n",
    "print(f\"\\nExample reconstruction (last training row):\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    short = col.replace(\"Tenor : \", \"T\").replace(\"; Maturity : \", \"/M\")\n",
    "    print(f\"  {short:<12} → predicted: {pred_original[0, i]:.6f} \"\n",
    "          f\"| true: {Y_raw[-1, i]:.6f}\")\n",
    "print(f\"  ... (showing 5 of {len(target_cols)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c1c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved → qrc_level2_model.pt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state\" : model.state_dict(),\n",
    "    \"pca\"         : pca,\n",
    "    \"pca_scaler\"  : pca_scaler,\n",
    "    \"x_scaler\"    : x_scaler,\n",
    "    \"y_scaler\"    : y_scaler,\n",
    "    \"input_cols\"  : input_cols,\n",
    "    \"target_cols\" : target_cols,\n",
    "    \"config\": {\n",
    "        \"TARGET_MATURITIES\" : TARGET_MATURITIES,\n",
    "        \"N_PCA_COMPONENTS\"  : N_PCA_COMPONENTS,\n",
    "        \"N_MODES\"           : N_MODES,\n",
    "        \"N_PHOTONS\"         : N_PHOTONS,\n",
    "    }\n",
    "}, \"qrc_level2_model.pt\")\n",
    "\n",
    "print(\"\\nModel saved → qrc_level2_model.pt\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
