{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4615349b",
   "metadata": {},
   "source": [
    "QML Model — Level 1: Future Swaption Surface Prediction\n",
    "Architecture: Quantum Reservoir Computing (QRC) with MerLin\n",
    " \n",
    "Pipeline:\n",
    " \n",
    "     1. Load & preprocess Level 1 data\n",
    " \n",
    "     2. Reduce 224 features → N via PCA (quantum circuits have mode limits)\n",
    " \n",
    "     3. Fixed quantum reservoir (MerLin) extracts non-linear features\n",
    " \n",
    "     4. Trainable classical linear readout predicts next-day's full 224-dim surface\n",
    " \n",
    "     5. Train, evaluate, save predictions\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513ba133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting merlinquantum\n",
      "  Downloading merlinquantum-0.3.1-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from merlinquantum) (2.10.0+cpu)\n",
      "Collecting perceval-quandela>=0.13.1 (from merlinquantum)\n",
      "  Downloading perceval_quandela-1.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpy>=2.2.6 (from merlinquantum)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from merlinquantum) (2.2.2)\n",
      "Collecting scikit-learn>=1.7.2 (from merlinquantum)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy~=1.12 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.14.0)\n",
      "Requirement already satisfied: scipy~=1.13 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.16.3)\n",
      "Requirement already satisfied: tabulate~=0.9 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (0.9.0)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (3.10.0)\n",
      "Collecting exqalibur~=1.1.0 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading exqalibur-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (430 bytes)\n",
      "Requirement already satisfied: multipledispatch<2 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (5.29.6)\n",
      "Collecting drawsvg>=2.0 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading drawsvg-2.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests<3 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (2.32.4)\n",
      "Requirement already satisfied: networkx<4,>=3.1 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (3.6.1)\n",
      "Collecting latexcodec<4 (from perceval-quandela>=0.13.1->merlinquantum)\n",
      "  Downloading latexcodec-3.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5 in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (4.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from perceval-quandela>=0.13.1->merlinquantum) (4.67.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->merlinquantum) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.7.2->merlinquantum) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (75.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->merlinquantum) (2025.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->merlinquantum) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->perceval-quandela>=0.13.1->merlinquantum) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->merlinquantum) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3->perceval-quandela>=0.13.1->merlinquantum) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy~=1.12->perceval-quandela>=0.13.1->merlinquantum) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->merlinquantum) (3.0.3)\n",
      "Downloading merlinquantum-0.3.1-py3-none-any.whl (197 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.0/197.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading perceval_quandela-1.1.0-py3-none-any.whl (432 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.8/432.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading drawsvg-2.4.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading exqalibur-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading latexcodec-3.0.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: drawsvg, numpy, latexcodec, exqalibur, scikit-learn, perceval-quandela, merlinquantum\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed drawsvg-2.4.1 exqalibur-1.1.1 latexcodec-3.0.1 merlinquantum-0.3.1 numpy-2.4.2 perceval-quandela-1.1.0 scikit-learn-1.8.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "85596dea31fd496ea624ffef3896c2d8",
       "pip_warning": {
        "packages": [
         "numpy",
         "sklearn"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install merlinquantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcedfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datasets import load_dataset\n",
    "\n",
    "import merlin as ML\n",
    "from merlin import LexGrouping, MeasurementStrategy, ComputationSpace\n",
    "from merlin.builder import CircuitBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175fae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PCA_COMPONENTS  = 16    # PCA: 224 → 16 (fits quantum mode limit)\n",
    "LOOKBACK          = 5     # Days of history per sample → input = 5×16 = 80\n",
    "N_MODES           = 16    # Quantum circuit modes (≤ 20 QPU hard limit)\n",
    "N_PHOTONS         = 4     # Photons in the register\n",
    "N_GROUPED_OUTPUTS = 32    # LexGrouping: compress Fock space → 32 features\n",
    "TRAIN_SPLIT       = 0.85\n",
    "EPOCHS            = 80\n",
    "LR                = 5e-4\n",
    "BATCH_SIZE        = 16\n",
    "DEVICE            = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e85b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance: 100.0%\n",
      "X shape: (489, 80)  (5 days × 16 PCA = 80 features)\n",
      "Y shape: (489, 224)\n",
      "\n",
      "Train: 415 samples | Val: 74 samples\n",
      "\n",
      "Quantum layer Fock output size : 1820\n",
      "After LexGrouping              : 32\n",
      "\n",
      "Total params     : 134,414\n",
      "Trainable params : 134,414\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2. PREPROCESS\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# MinMax scale to [0, 1] — required for angle encoding stability (MerLin docs)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(raw_data).astype(np.float32)\n",
    "\n",
    "# PCA: 224 → 16\n",
    "pca = PCA(n_components=N_PCA_COMPONENTS)\n",
    "data_pca = pca.fit_transform(data_scaled).astype(np.float32)\n",
    "\n",
    "# Re-scale PCA outputs to [0, 1] so the pre-compression Sigmoid stays well-behaved\n",
    "pca_scaler = MinMaxScaler()\n",
    "data_pca = pca_scaler.fit_transform(data_pca).astype(np.float32)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3. BUILD LOOKBACK WINDOWS\n",
    "# ─────────────────────────────────────────────\n",
    "# Input:  last LOOKBACK days of PCA surface, flattened → (80,)\n",
    "# Target: full 224-dim surface on the next day\n",
    "\n",
    "N = len(data_pca)\n",
    "X_list, Y_list = [], []\n",
    "for i in range(LOOKBACK, N):\n",
    "    window = data_pca[i - LOOKBACK:i].flatten()\n",
    "    X_list.append(window)\n",
    "    Y_list.append(data_scaled[i])\n",
    "\n",
    "X = np.array(X_list, dtype=np.float32)   # (N-LOOKBACK, 80)\n",
    "Y = np.array(Y_list, dtype=np.float32)   # (N-LOOKBACK, 224)\n",
    "\n",
    "print(f\"X shape: {X.shape}  ({LOOKBACK} days × {N_PCA_COMPONENTS} PCA = {X.shape[1]} features)\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 4. TRAIN / VAL SPLIT  (chronological — never shuffle time series)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "split    = int(len(X) * TRAIN_SPLIT)\n",
    "X_train  = torch.tensor(X[:split], device=DEVICE)\n",
    "Y_train  = torch.tensor(Y[:split], device=DEVICE)\n",
    "X_val    = torch.tensor(X[split:], device=DEVICE)\n",
    "Y_val    = torch.tensor(Y[split:], device=DEVICE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} samples | Val: {len(X_val)} samples\")\n",
    "\n",
    "builder = CircuitBuilder(n_modes=N_MODES)\n",
    "builder.add_entangling_layer(trainable=True, name=\"U1\")\n",
    "builder.add_angle_encoding(\n",
    "    modes=list(range(N_MODES)),\n",
    "    name=\"input\",\n",
    "    scale=np.pi,\n",
    ")\n",
    "builder.add_rotations(trainable=True, name=\"theta\")\n",
    "builder.add_superpositions(depth=1, trainable=True)\n",
    "\n",
    "quantum_core = ML.QuantumLayer(\n",
    "    input_size=N_MODES,\n",
    "    builder=builder,\n",
    "    n_photons=N_PHOTONS,\n",
    "    measurement_strategy=MeasurementStrategy.probs(ComputationSpace.UNBUNCHED),\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantum layer Fock output size : {quantum_core.output_size}\")\n",
    "print(f\"After LexGrouping              : {N_GROUPED_OUTPUTS}\")\n",
    "\n",
    "class QRCSwaption(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classical pre-compression: squeeze lookback window → quantum-compatible size\n",
    "        # Sigmoid ensures output stays in [0,1] for angle encoding\n",
    "        self.pre_compress = nn.Sequential(\n",
    "            nn.Linear(input_size, N_MODES),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Quantum feature extraction + Fock space grouping\n",
    "        self.quantum = nn.Sequential(\n",
    "            quantum_core,\n",
    "            LexGrouping(quantum_core.output_size, N_GROUPED_OUTPUTS),\n",
    "        )\n",
    "\n",
    "        # Classical readout with regularization\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(N_GROUPED_OUTPUTS, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pre_compress(x)   # (B, 80) → (B, 16)\n",
    "        x = self.quantum(x)        # (B, 16) → (B, 32)\n",
    "        return self.readout(x)     # (B, 32) → (B, 224)\n",
    "\n",
    "\n",
    "model = QRCSwaption(\n",
    "    input_size=N_PCA_COMPONENTS * LOOKBACK,\n",
    "    output_size=len(feature_cols),\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal params     : {total_params:,}\")\n",
    "print(f\"Trainable params : {trainable_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa935126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLIKE loss ready ✓  (sanity check passed: QLIKE(x,x) ≈ 0)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# QLIKE LOSS FUNCTION\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# QLIKE (Quasi-Likelihood) is the standard loss for volatility forecasting.\n",
    "# It is already inherently asymmetric and scale-sensitive:\n",
    "#   - Underpredicting volatility is penalised MORE than overpredicting\n",
    "#   - A 0.01 error on vol=0.03 (short maturity) is penalised far more\n",
    "#     than the same error on vol=0.35 (long maturity)\n",
    "#\n",
    "# We tried explicit maturity-based weighting (1/maturity) but it was\n",
    "# too aggressive — the gradient became dominated by short-maturity cells\n",
    "# before the model had learned the overall surface, degrading everything.\n",
    "# Plain QLIKE is already the right inductive bias for this problem.\n",
    "#\n",
    "# Formula (volatility form):\n",
    "#   QLIKE(σ_true, σ_pred) = mean( (σ_true/σ_pred)² - 2·log(σ_true/σ_pred) - 1 )\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def qlike_loss(pred: 'torch.Tensor', target: 'torch.Tensor') -> 'torch.Tensor':\n",
    "    \"\"\"\n",
    "    QLIKE loss in volatility form, computed on MinMax-scaled values.\n",
    "    Both pred and target are in [0,1].\n",
    "    Returns a scalar tensor suitable for .backward().\n",
    "    \"\"\"\n",
    "    pred   = pred.clamp(min=EPS)\n",
    "    target = target.clamp(min=EPS)\n",
    "    ratio  = target / pred\n",
    "    return (ratio ** 2 - 2 * torch.log(ratio) - 1).mean()\n",
    "\n",
    "\n",
    "def qlike_numpy(true: 'np.ndarray', pred: 'np.ndarray') -> float:\n",
    "    \"\"\"\n",
    "    QLIKE in original volatility space — used for final evaluation reporting.\n",
    "    \"\"\"\n",
    "    pred  = np.clip(pred,  EPS, None)\n",
    "    true  = np.clip(true,  EPS, None)\n",
    "    ratio = true / pred\n",
    "    return float(np.mean(ratio ** 2 - 2 * np.log(ratio) - 1))\n",
    "\n",
    "\n",
    "# Sanity check: perfect predictions should give QLIKE ≈ 0\n",
    "x = torch.full((4, len(feature_cols)), 0.2)\n",
    "assert qlike_loss(x, x).item() < 1e-4, \"QLIKE(x,x) should be ≈ 0\"\n",
    "print(\"QLIKE loss ready ✓  (sanity check passed: QLIKE(x,x) ≈ 0)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a13c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "  Epoch   1/80 | train QLIKE: 0.496345 | val QLIKE: 0.870009 | best QLIKE: 0.870009\n",
      "  Epoch   2/80 | train QLIKE: 0.429749 | val QLIKE: 0.665830 | best QLIKE: 0.665830\n",
      "  Epoch   3/80 | train QLIKE: 0.440207 | val QLIKE: 0.941472 | best QLIKE: 0.665830\n",
      "  Epoch   4/80 | train QLIKE: 0.429473 | val QLIKE: 0.755854 | best QLIKE: 0.665830\n",
      "  Epoch   5/80 | train QLIKE: 0.421592 | val QLIKE: 0.756206 | best QLIKE: 0.665830\n",
      "  Epoch   6/80 | train QLIKE: 0.421449 | val QLIKE: 0.867939 | best QLIKE: 0.665830\n",
      "  Epoch   7/80 | train QLIKE: 0.417985 | val QLIKE: 0.911400 | best QLIKE: 0.665830\n",
      "  Epoch   8/80 | train QLIKE: 0.405305 | val QLIKE: 0.823381 | best QLIKE: 0.665830\n",
      "  Epoch   9/80 | train QLIKE: 0.404871 | val QLIKE: 0.928549 | best QLIKE: 0.665830\n",
      "  Epoch  10/80 | train QLIKE: 0.401696 | val QLIKE: 0.865875 | best QLIKE: 0.665830\n",
      "  Epoch  11/80 | train QLIKE: 0.406333 | val QLIKE: 1.125973 | best QLIKE: 0.665830\n",
      "  Epoch  12/80 | train QLIKE: 0.388053 | val QLIKE: 0.705651 | best QLIKE: 0.665830\n",
      "  Epoch  13/80 | train QLIKE: 0.371697 | val QLIKE: 0.748018 | best QLIKE: 0.665830\n",
      "  Epoch  14/80 | train QLIKE: 0.368445 | val QLIKE: 0.877880 | best QLIKE: 0.665830\n",
      "  Epoch  15/80 | train QLIKE: 0.369804 | val QLIKE: 0.802801 | best QLIKE: 0.665830\n",
      "  Epoch  16/80 | train QLIKE: 0.362022 | val QLIKE: 0.744808 | best QLIKE: 0.665830\n",
      "  Epoch  17/80 | train QLIKE: 0.357145 | val QLIKE: 0.783129 | best QLIKE: 0.665830\n",
      "  Epoch  18/80 | train QLIKE: 0.365537 | val QLIKE: 0.791201 | best QLIKE: 0.665830\n",
      "  Epoch  19/80 | train QLIKE: 0.350466 | val QLIKE: 0.766773 | best QLIKE: 0.665830\n",
      "  Epoch  20/80 | train QLIKE: 0.349111 | val QLIKE: 0.781925 | best QLIKE: 0.665830\n",
      "  Epoch  21/80 | train QLIKE: 0.340859 | val QLIKE: 0.768153 | best QLIKE: 0.665830\n",
      "  Epoch  22/80 | train QLIKE: 0.343452 | val QLIKE: 0.748471 | best QLIKE: 0.665830\n",
      "  Epoch  23/80 | train QLIKE: 0.346773 | val QLIKE: 0.812787 | best QLIKE: 0.665830\n",
      "  Epoch  24/80 | train QLIKE: 0.333704 | val QLIKE: 0.779602 | best QLIKE: 0.665830\n",
      "  Epoch  25/80 | train QLIKE: 0.341414 | val QLIKE: 0.822021 | best QLIKE: 0.665830\n",
      "  Epoch  26/80 | train QLIKE: 0.337760 | val QLIKE: 0.775627 | best QLIKE: 0.665830\n",
      "  Epoch  27/80 | train QLIKE: 0.335324 | val QLIKE: 0.792722 | best QLIKE: 0.665830\n",
      "  Epoch  28/80 | train QLIKE: 0.328440 | val QLIKE: 0.776449 | best QLIKE: 0.665830\n",
      "  Epoch  29/80 | train QLIKE: 0.331525 | val QLIKE: 0.799609 | best QLIKE: 0.665830\n",
      "  Epoch  30/80 | train QLIKE: 0.327634 | val QLIKE: 0.805437 | best QLIKE: 0.665830\n",
      "  Epoch  31/80 | train QLIKE: 0.328614 | val QLIKE: 0.773359 | best QLIKE: 0.665830\n",
      "  Epoch  32/80 | train QLIKE: 0.323057 | val QLIKE: 0.764029 | best QLIKE: 0.665830\n",
      "  Epoch  33/80 | train QLIKE: 0.321590 | val QLIKE: 0.750077 | best QLIKE: 0.665830\n",
      "  Epoch  34/80 | train QLIKE: 0.329714 | val QLIKE: 0.800611 | best QLIKE: 0.665830\n",
      "  Epoch  35/80 | train QLIKE: 0.323799 | val QLIKE: 0.791007 | best QLIKE: 0.665830\n",
      "  Epoch  36/80 | train QLIKE: 0.324346 | val QLIKE: 0.780491 | best QLIKE: 0.665830\n",
      "  Epoch  37/80 | train QLIKE: 0.317641 | val QLIKE: 0.776047 | best QLIKE: 0.665830\n",
      "  Epoch  38/80 | train QLIKE: 0.315326 | val QLIKE: 0.782640 | best QLIKE: 0.665830\n",
      "  Epoch  39/80 | train QLIKE: 0.319404 | val QLIKE: 0.779437 | best QLIKE: 0.665830\n",
      "  Epoch  40/80 | train QLIKE: 0.315331 | val QLIKE: 0.767077 | best QLIKE: 0.665830\n",
      "  Epoch  41/80 | train QLIKE: 0.319592 | val QLIKE: 0.773418 | best QLIKE: 0.665830\n",
      "  Epoch  42/80 | train QLIKE: 0.322931 | val QLIKE: 0.779859 | best QLIKE: 0.665830\n",
      "  Epoch  43/80 | train QLIKE: 0.314218 | val QLIKE: 0.773784 | best QLIKE: 0.665830\n",
      "  Epoch  44/80 | train QLIKE: 0.315955 | val QLIKE: 0.797847 | best QLIKE: 0.665830\n",
      "  Epoch  45/80 | train QLIKE: 0.312840 | val QLIKE: 0.760114 | best QLIKE: 0.665830\n",
      "  Epoch  46/80 | train QLIKE: 0.311892 | val QLIKE: 0.769588 | best QLIKE: 0.665830\n",
      "  Epoch  47/80 | train QLIKE: 0.319583 | val QLIKE: 0.790537 | best QLIKE: 0.665830\n",
      "  Epoch  48/80 | train QLIKE: 0.322708 | val QLIKE: 0.780674 | best QLIKE: 0.665830\n",
      "  Epoch  49/80 | train QLIKE: 0.317213 | val QLIKE: 0.780687 | best QLIKE: 0.665830\n",
      "  Epoch  50/80 | train QLIKE: 0.313366 | val QLIKE: 0.779101 | best QLIKE: 0.665830\n",
      "  Epoch  51/80 | train QLIKE: 0.320994 | val QLIKE: 0.774700 | best QLIKE: 0.665830\n",
      "  Epoch  52/80 | train QLIKE: 0.313378 | val QLIKE: 0.784951 | best QLIKE: 0.665830\n",
      "  Epoch  53/80 | train QLIKE: 0.313990 | val QLIKE: 0.777183 | best QLIKE: 0.665830\n",
      "  Epoch  54/80 | train QLIKE: 0.318190 | val QLIKE: 0.780570 | best QLIKE: 0.665830\n",
      "  Epoch  55/80 | train QLIKE: 0.314718 | val QLIKE: 0.773759 | best QLIKE: 0.665830\n",
      "  Epoch  56/80 | train QLIKE: 0.315900 | val QLIKE: 0.776462 | best QLIKE: 0.665830\n",
      "  Epoch  57/80 | train QLIKE: 0.314782 | val QLIKE: 0.771344 | best QLIKE: 0.665830\n",
      "  Epoch  58/80 | train QLIKE: 0.307108 | val QLIKE: 0.783778 | best QLIKE: 0.665830\n",
      "  Epoch  59/80 | train QLIKE: 0.316831 | val QLIKE: 0.778472 | best QLIKE: 0.665830\n",
      "  Epoch  60/80 | train QLIKE: 0.310053 | val QLIKE: 0.775987 | best QLIKE: 0.665830\n",
      "  Epoch  61/80 | train QLIKE: 0.315955 | val QLIKE: 0.772909 | best QLIKE: 0.665830\n",
      "  Epoch  62/80 | train QLIKE: 0.310098 | val QLIKE: 0.784459 | best QLIKE: 0.665830\n",
      "  Epoch  63/80 | train QLIKE: 0.319245 | val QLIKE: 0.777456 | best QLIKE: 0.665830\n",
      "  Epoch  64/80 | train QLIKE: 0.320693 | val QLIKE: 0.773991 | best QLIKE: 0.665830\n",
      "  Epoch  65/80 | train QLIKE: 0.323019 | val QLIKE: 0.777642 | best QLIKE: 0.665830\n",
      "  Epoch  66/80 | train QLIKE: 0.314733 | val QLIKE: 0.783454 | best QLIKE: 0.665830\n",
      "  Epoch  67/80 | train QLIKE: 0.318216 | val QLIKE: 0.786236 | best QLIKE: 0.665830\n",
      "  Epoch  68/80 | train QLIKE: 0.306569 | val QLIKE: 0.767652 | best QLIKE: 0.665830\n",
      "  Epoch  69/80 | train QLIKE: 0.313731 | val QLIKE: 0.771332 | best QLIKE: 0.665830\n",
      "  Epoch  70/80 | train QLIKE: 0.322201 | val QLIKE: 0.776214 | best QLIKE: 0.665830\n",
      "  Epoch  71/80 | train QLIKE: 0.307645 | val QLIKE: 0.783213 | best QLIKE: 0.665830\n",
      "  Epoch  72/80 | train QLIKE: 0.317584 | val QLIKE: 0.779541 | best QLIKE: 0.665830\n",
      "  Epoch  73/80 | train QLIKE: 0.313431 | val QLIKE: 0.779015 | best QLIKE: 0.665830\n",
      "  Epoch  74/80 | train QLIKE: 0.314999 | val QLIKE: 0.767851 | best QLIKE: 0.665830\n",
      "  Epoch  75/80 | train QLIKE: 0.320324 | val QLIKE: 0.774223 | best QLIKE: 0.665830\n",
      "  Epoch  76/80 | train QLIKE: 0.311215 | val QLIKE: 0.789787 | best QLIKE: 0.665830\n",
      "  Epoch  77/80 | train QLIKE: 0.319538 | val QLIKE: 0.781644 | best QLIKE: 0.665830\n",
      "  Epoch  78/80 | train QLIKE: 0.312915 | val QLIKE: 0.785620 | best QLIKE: 0.665830\n",
      "  Epoch  79/80 | train QLIKE: 0.310956 | val QLIKE: 0.771854 | best QLIKE: 0.665830\n",
      "  Epoch  80/80 | train QLIKE: 0.307234 | val QLIKE: 0.778214 | best QLIKE: 0.665830\n",
      "\n",
      "Restored best model (best QLIKE: 0.665830)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 7. TRAIN\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Halve LR when val loss stops improving — helps avoid getting stuck\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=8, \n",
    ")\n",
    "\n",
    "loss_fn = qlike_loss   # QLIKE: penalises underprediction more than overprediction\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state    = None\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(xb)\n",
    "\n",
    "    epoch_loss /= len(X_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = loss_fn(model(X_val), Y_val).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "\n",
    "    print(f\"  Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"train QLIKE: {epoch_loss:.6f} | \"\n",
    "              f\"val QLIKE: {val_loss:.6f} | \"\n",
    "              f\"best QLIKE: {best_val_loss:.6f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nRestored best model (best QLIKE: {best_val_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d88e5b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "VALIDATION RESULTS (original volatility scale)\n",
      "=======================================================\n",
      "  Overall RMSE  : 0.014969\n",
      "  Overall MAE   : 0.009463\n",
      "  Overall QLIKE : 0.012683   ← primary metric\n",
      "  (Volatility range ≈ 0.02 – 0.45)\n",
      "\n",
      "  Per-maturity QLIKE breakdown:\n",
      "    Maturity  0.08yr → QLIKE: 0.032787\n",
      "    Maturity  0.25yr → QLIKE: 0.028769\n",
      "    Maturity  0.50yr → QLIKE: 0.019987\n",
      "    Maturity  0.75yr → QLIKE: 0.018183\n",
      "    Maturity  1.00yr → QLIKE: 0.015371\n",
      "    Maturity  1.50yr → QLIKE: 0.016049\n",
      "    Maturity  2.00yr → QLIKE: 0.015862\n",
      "    Maturity  3.00yr → QLIKE: 0.012260\n",
      "    Maturity  4.00yr → QLIKE: 0.010339\n",
      "    Maturity  5.00yr → QLIKE: 0.008111\n",
      "    Maturity  7.00yr → QLIKE: 0.004641\n",
      "    Maturity 10.00yr → QLIKE: 0.002599\n",
      "    Maturity 15.00yr → QLIKE: 0.002862\n",
      "    Maturity 20.00yr → QLIKE: 0.004283\n",
      "    Maturity 25.00yr → QLIKE: 0.004221\n",
      "    Maturity 30.00yr → QLIKE: 0.006599\n",
      "\n",
      "Predicted next-day surface (first 5 values): [0.02570629 0.03446506 0.03836861 0.04053444 0.04163408]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 8. EVALUATE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred_np = model(X_val).numpy()\n",
    "\n",
    "val_true_np       = Y_val.numpy()\n",
    "val_pred_original = scaler.inverse_transform(val_pred_np)\n",
    "val_true_original = scaler.inverse_transform(val_true_np)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(val_true_original, val_pred_original))\n",
    "mae  = np.mean(np.abs(val_true_original - val_pred_original))\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"VALIDATION RESULTS (original volatility scale)\")\n",
    "print(f\"{'='*55}\")\n",
    "qlike_val = qlike_numpy(val_true_original, val_pred_original)\n",
    "\n",
    "print(f\"  Overall RMSE  : {rmse:.6f}\")\n",
    "print(f\"  Overall MAE   : {mae:.6f}\")\n",
    "print(f\"  Overall QLIKE : {qlike_val:.6f}   ← primary metric\")\n",
    "print(f\"  (Volatility range ≈ 0.02 – 0.45)\")\n",
    "\n",
    "# Per-maturity QLIKE breakdown\n",
    "# Short maturities (low vol) are weighted more heavily by QLIKE\n",
    "maturities_list = sorted(set(\n",
    "    float(c.split(\"Maturity : \")[1]) for c in feature_cols\n",
    "))\n",
    "print(f\"\\n  Per-maturity QLIKE breakdown:\")\n",
    "for mat in maturities_list:\n",
    "    mat_idx = [i for i, c in enumerate(feature_cols)\n",
    "               if float(c.split(\"Maturity : \")[1]) == mat]\n",
    "    q = qlike_numpy(val_true_original[:, mat_idx], val_pred_original[:, mat_idx])\n",
    "    print(f\"    Maturity {mat:5.2f}yr → QLIKE: {q:.6f}\")\n",
    "\n",
    "last_window_raw    = raw_data[-LOOKBACK:]\n",
    "last_window_scaled = scaler.transform(last_window_raw)\n",
    "last_window_pca    = pca.transform(last_window_scaled)\n",
    "last_window_pca    = pca_scaler.transform(last_window_pca).astype(np.float32)\n",
    "last_window_flat   = last_window_pca.flatten()[np.newaxis, :]    # (1, 80)\n",
    "last_day_tensor    = torch.tensor(last_window_flat, device=DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    next_day_scaled = model(last_day_tensor).numpy()\n",
    "\n",
    "next_day_pred = scaler.inverse_transform(next_day_scaled)\n",
    "print(f\"\\nPredicted next-day surface (first 5 values): {next_day_pred[0, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e42b0e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "\n",
      "Model saved → qrc_swaption_model.pt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 10. SAVE\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"test\")\n",
    "torch.save({\n",
    "    \"model_state\"  : model.state_dict(),\n",
    "    \"pca\"          : pca,\n",
    "    \"pca_scaler\"   : pca_scaler,\n",
    "    \"scaler\"       : scaler,\n",
    "    \"feature_cols\" : feature_cols,\n",
    "    \"config\": {\n",
    "        \"N_PCA_COMPONENTS\"  : N_PCA_COMPONENTS,\n",
    "        \"LOOKBACK\"          : LOOKBACK,\n",
    "        \"N_MODES\"           : N_MODES,\n",
    "        \"N_PHOTONS\"         : N_PHOTONS,\n",
    "        \"N_GROUPED_OUTPUTS\" : N_GROUPED_OUTPUTS,\n",
    "    }\n",
    "}, \"qrc_swaption_model.pt\")\n",
    "\n",
    "print(\"\\nModel saved → qrc_swaption_model.pt\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb9dac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at: /content/qrc_swaption_model.pt\n",
      "File size: 0.6 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Show where it saved\n",
    "print(f\"Saved at: {os.path.abspath('qrc_swaption_model.pt')}\")\n",
    "print(f\"File size: {os.path.getsize('qrc_swaption_model.pt') / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e4bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a329e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Google Drive → MyDrive/qrc_swaption_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save directly to Drive\n",
    "import shutil\n",
    "torch.save({\n",
    "    \"model_state\"  : model.state_dict(),\n",
    "    \"pca\"          : pca,\n",
    "    \"pca_scaler\"   : pca_scaler,\n",
    "    \"scaler\"       : scaler,\n",
    "    \"feature_cols\" : feature_cols,\n",
    "    \"config\": {\n",
    "        \"N_PCA_COMPONENTS\"  : N_PCA_COMPONENTS,\n",
    "        \"LOOKBACK\"          : LOOKBACK,\n",
    "        \"N_MODES\"           : N_MODES,\n",
    "        \"N_PHOTONS\"         : N_PHOTONS,\n",
    "        \"N_GROUPED_OUTPUTS\" : N_GROUPED_OUTPUTS,\n",
    "    }\n",
    "}, \"qrc_swaption_model.pt\")\n",
    "\n",
    "# Copy to Drive\n",
    "shutil.copy(\"qrc_swaption_model.pt\", \"/content/drive/MyDrive/qrc_swaption_model.pt\")\n",
    "print(\"Saved to Google Drive → MyDrive/qrc_swaption_model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
