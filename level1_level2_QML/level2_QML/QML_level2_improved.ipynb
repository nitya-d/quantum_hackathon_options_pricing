{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6010146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nQML Model — Level 2: Swaption Surface Missing Cell Reconstruction\\nArchitecture: Quantum Masked Autoencoder with MerLin CircuitBuilder\\n\\nProblem (corrected understanding):\\n    The test set has ARBITRARY individual cells missing — not full maturity rows.\\n    Any (tenor, maturity) combination can be missing.\\n    e.g. \"Tenor:5; Maturity:0.083\", \"Tenor:15; Maturity:0.25\", \"Tenor:10; Maturity:0.5\"\\n\\n    The training data is fully observed — we simulate missing cells by randomly\\n    masking cells during training (masked autoencoder approach).\\n\\nPipeline:\\n    1. Load Level 2 training data (fully observed, 489 rows × 224 cells)\\n    2. Each training step: randomly mask M cells → set to 0\\n    3. Input  = masked surface (224) + binary mask (224) → 448 features\\n    4. PCA compress 448 → 16 + re-scale to [0,1]\\n    5. Quantum circuit (CircuitBuilder, 16 modes, 4 photons)\\n    6. LexGrouping → 32 features\\n    7. Classical readout → 224 (full surface reconstruction)\\n    8. Loss computed ONLY on the masked cells (so model learns to impute)\\n    9. At test time: fill NaN cells with 0, build mask, run model,\\n       use predictions for the NaN positions only\\n\\nKey insight:\\n    The binary mask is essential — it tells the quantum circuit WHICH cells\\n    are observed vs missing, preventing the model from confusing \"zero volatility\"\\n    with \"missing value\".\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "QML Model — Level 2: Swaption Surface Missing Cell Reconstruction\n",
    "Architecture: Quantum Masked Autoencoder with MerLin CircuitBuilder\n",
    "\n",
    "Problem (corrected understanding):\n",
    "    The test set has ARBITRARY individual cells missing — not full maturity rows.\n",
    "    Any (tenor, maturity) combination can be missing.\n",
    "    e.g. \"Tenor:5; Maturity:0.083\", \"Tenor:15; Maturity:0.25\", \"Tenor:10; Maturity:0.5\"\n",
    "\n",
    "    The training data is fully observed — we simulate missing cells by randomly\n",
    "    masking cells during training (masked autoencoder approach).\n",
    "\n",
    "Pipeline:\n",
    "    1. Load Level 2 training data (fully observed, 489 rows × 224 cells)\n",
    "    2. Each training step: randomly mask M cells → set to 0\n",
    "    3. Input  = masked surface (224) + binary mask (224) → 448 features\n",
    "    4. PCA compress 448 → 16 + re-scale to [0,1]\n",
    "    5. Quantum circuit (CircuitBuilder, 16 modes, 4 photons)\n",
    "    6. LexGrouping → 32 features\n",
    "    7. Classical readout → 224 (full surface reconstruction)\n",
    "    8. Loss computed ONLY on the masked cells (so model learns to impute)\n",
    "    9. At test time: fill NaN cells with 0, build mask, run model,\n",
    "       use predictions for the NaN positions only\n",
    "\n",
    "Key insight:\n",
    "    The binary mask is essential — it tells the quantum circuit WHICH cells\n",
    "    are observed vs missing, preventing the model from confusing \"zero volatility\"\n",
    "    with \"missing value\".\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe66f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datasets import load_dataset\n",
    "\n",
    "import merlin as ML\n",
    "from merlin import LexGrouping, MeasurementStrategy, ComputationSpace\n",
    "from merlin.builder import CircuitBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06912a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PCA_COMPONENTS  = 16    # PCA: 448 (surface + mask) → 16\n",
    "N_MODES           = 16    # Quantum circuit modes (≤ 20 QPU hard limit)\n",
    "N_PHOTONS         = 4\n",
    "N_GROUPED_OUTPUTS = 32    # LexGrouping output\n",
    "MASK_RATIO        = 0.15  # Randomly mask 15% of cells per training sample\n",
    "                          # (~34 cells out of 224) — simulates realistic missing patterns\n",
    "TRAIN_SPLIT       = 0.85\n",
    "EPOCHS            = 100\n",
    "LR                = 5e-4\n",
    "BATCH_SIZE        = 16\n",
    "DEVICE            = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cdde39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Level 2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (489, 224)\n",
      "Feature columns: 224\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 1. LOAD DATA\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "print(\"Loading Level 2 dataset...\")\n",
    "ds = load_dataset(\n",
    "    \"Quandela/Challenge_Swaptions\",\n",
    "    data_files=\"level-2_Missing_data_prediction/train_level2.csv\",\n",
    "    split=\"train\",\n",
    ")\n",
    "df = ds.to_pandas()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c != \"Date\"]\n",
    "raw_data = df[feature_cols].values.astype(np.float32)  # (489, 224)\n",
    "\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f956d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface scaled to [0,1]\n",
      "\n",
      "Masked input shape : (489, 448)  (224 surface + 224 mask)\n",
      "Target shape       : (489, 224)\n",
      "Avg cells masked   : 33 / 224 (15%)\n",
      "\n",
      "PCA explained variance: 29.0%\n",
      "X after PCA: (489, 16)\n",
      "\n",
      "Train: 415 samples | Val: 74 samples\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2. SCALE THE SURFACE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# MinMaxScaler on the surface values → [0, 1]\n",
    "# Important: masked cells will be set to 0, which is a valid value in this range\n",
    "# so the mask channel is essential to distinguish \"truly zero\" from \"missing\"\n",
    "surface_scaler = MinMaxScaler()\n",
    "data_scaled = surface_scaler.fit_transform(raw_data).astype(np.float32)  # (489, 224)\n",
    "\n",
    "print(f\"Surface scaled to [0,1]\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3. BUILD MASKED TRAINING PAIRS\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# For each row we create: (masked_surface + mask, full_surface)\n",
    "# The mask is a binary vector: 1 = observed, 0 = missing\n",
    "# We stack [masked_surface | mask] → 448-dim input\n",
    "#\n",
    "# We pre-generate all masked samples here (one mask per row).\n",
    "# At each epoch we could re-generate for more variety, but pre-generation\n",
    "# is simpler and still effective for 489 rows.\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples  = len(data_scaled)\n",
    "n_features = data_scaled.shape[1]  # 224\n",
    "\n",
    "masked_inputs = np.zeros((n_samples, n_features * 2), dtype=np.float32)\n",
    "full_targets  = data_scaled.copy()\n",
    "\n",
    "mask_indices_list = []  # store which cells were masked per row (for loss computation)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    row = data_scaled[i].copy()\n",
    "\n",
    "    # Randomly select cells to mask\n",
    "    n_mask = max(1, int(n_features * MASK_RATIO))\n",
    "    mask_idx = np.random.choice(n_features, n_mask, replace=False)\n",
    "    mask_indices_list.append(mask_idx)\n",
    "\n",
    "    # Binary mask: 1=observed, 0=missing\n",
    "    mask = np.ones(n_features, dtype=np.float32)\n",
    "    mask[mask_idx] = 0.0\n",
    "\n",
    "    # Zero out the masked cells\n",
    "    masked_row = row.copy()\n",
    "    masked_row[mask_idx] = 0.0\n",
    "\n",
    "    # Concatenate: [masked_surface (224) | binary_mask (224)] → (448,)\n",
    "    masked_inputs[i] = np.concatenate([masked_row, mask])\n",
    "\n",
    "print(f\"\\nMasked input shape : {masked_inputs.shape}  (224 surface + 224 mask)\")\n",
    "print(f\"Target shape       : {full_targets.shape}\")\n",
    "print(f\"Avg cells masked   : {int(n_features * MASK_RATIO)} / {n_features} \"\n",
    "      f\"({MASK_RATIO*100:.0f}%)\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 4. PCA ON THE MASKED INPUTS\n",
    "# ─────────────────────────────────────────────\n",
    "# PCA on 448 → 16 to fit quantum mode limit\n",
    "\n",
    "pca = PCA(n_components=N_PCA_COMPONENTS)\n",
    "X_pca = pca.fit_transform(masked_inputs).astype(np.float32)   # (489, 16)\n",
    "\n",
    "# Re-scale PCA outputs to [0,1] for angle encoding\n",
    "pca_scaler = MinMaxScaler()\n",
    "X_pca = pca_scaler.fit_transform(X_pca).astype(np.float32)\n",
    "\n",
    "print(f\"\\nPCA explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"X after PCA: {X_pca.shape}\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 5. TRAIN / VAL SPLIT  (chronological)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "split   = int(n_samples * TRAIN_SPLIT)\n",
    "\n",
    "X_train = torch.tensor(X_pca[:split],        device=DEVICE)\n",
    "Y_train = torch.tensor(full_targets[:split],  device=DEVICE)\n",
    "X_val   = torch.tensor(X_pca[split:],         device=DEVICE)\n",
    "Y_val   = torch.tensor(full_targets[split:],  device=DEVICE)\n",
    "\n",
    "# Store mask indices for computing loss only on masked cells\n",
    "mask_train = mask_indices_list[:split]\n",
    "mask_val   = mask_indices_list[split:]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train, torch.arange(split)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} samples | Val: {len(X_val)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6f901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum layer Fock output size : 1820\n",
      "After LexGrouping              : 32\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 6. BUILD THE QUANTUM CIRCUIT\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "builder = CircuitBuilder(n_modes=N_MODES)\n",
    "builder.add_entangling_layer(trainable=True, name=\"U1\")\n",
    "builder.add_angle_encoding(\n",
    "    modes=list(range(N_MODES)),\n",
    "    name=\"input\",\n",
    "    scale=np.pi,\n",
    ")\n",
    "builder.add_rotations(trainable=True, name=\"theta\")\n",
    "builder.add_superpositions(depth=2, trainable=True)\n",
    "\n",
    "quantum_core = ML.QuantumLayer(\n",
    "    input_size=N_MODES,\n",
    "    builder=builder,\n",
    "    n_photons=N_PHOTONS,\n",
    "    measurement_strategy=MeasurementStrategy.probs(ComputationSpace.UNBUNCHED),\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantum layer Fock output size : {quantum_core.output_size}\")\n",
    "print(f\"After LexGrouping              : {N_GROUPED_OUTPUTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35eae6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total params     : 133,148\n",
      "Trainable params : 133,148\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 7. FULL HYBRID MODEL\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "#   [16]  PCA of (masked surface + binary mask)\n",
    "#     │\n",
    "#   QuantumLayer (16 modes, 4 photons)\n",
    "#     │\n",
    "#   LexGrouping(Fock → 32)\n",
    "#     │\n",
    "#   Linear(32→256) + BN + ReLU + Dropout(0.3)\n",
    "#   Linear(256→256) + BN + ReLU + Dropout(0.3)\n",
    "#   Linear(256→224) + Sigmoid\n",
    "#     │\n",
    "#   [224] full reconstructed surface\n",
    "#        → at test time, only the missing cell predictions are used\n",
    "\n",
    "class QRCMaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.quantum = nn.Sequential(\n",
    "            quantum_core,\n",
    "            LexGrouping(quantum_core.output_size, N_GROUPED_OUTPUTS),\n",
    "        )\n",
    "\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(N_GROUPED_OUTPUTS, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.Sigmoid(),   # output in [0,1], same scale as MinMax-scaled targets\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quantum(x)\n",
    "        return self.readout(x)\n",
    "\n",
    "\n",
    "model = QRCMaskedAutoencoder(output_size=n_features).to(DEVICE)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal params     : {total_params:,}\")\n",
    "print(f\"Trainable params : {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254ef117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 8. CUSTOM LOSS — only on masked cells\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# We only penalize the model on cells it couldn't see.\n",
    "# Computing loss on observed cells too would be trivial (they're given)\n",
    "# and would drown out the imputation signal.\n",
    "\n",
    "def masked_mse_loss(pred, target, mask_indices_batch, batch_indices):\n",
    "    \"\"\"MSE computed only on the masked (missing) cells.\"\"\"\n",
    "    loss = torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "    count = 0\n",
    "    for j, global_idx in enumerate(batch_indices):\n",
    "        idx = mask_train[global_idx.item()]\n",
    "        loss = loss + ((pred[j, idx] - target[j, idx]) ** 2).mean()\n",
    "        count += 1\n",
    "    return loss / max(count, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7aad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (loss computed on masked cells only)...\n",
      "  Epoch   1/100 | train MSE: 0.063059 | val MSE (masked): 0.051280 | best: 0.051280\n",
      "  Epoch   2/100 | train MSE: 0.050502 | val MSE (masked): 0.044997 | best: 0.044997\n",
      "  Epoch   3/100 | train MSE: 0.046689 | val MSE (masked): 0.043441 | best: 0.043441\n",
      "  Epoch   4/100 | train MSE: 0.044442 | val MSE (masked): 0.036650 | best: 0.036650\n",
      "  Epoch   5/100 | train MSE: 0.042307 | val MSE (masked): 0.033283 | best: 0.033283\n",
      "  Epoch   6/100 | train MSE: 0.041009 | val MSE (masked): 0.030646 | best: 0.030646\n",
      "  Epoch   7/100 | train MSE: 0.039811 | val MSE (masked): 0.028870 | best: 0.028870\n",
      "  Epoch   8/100 | train MSE: 0.039084 | val MSE (masked): 0.027447 | best: 0.027447\n",
      "  Epoch   9/100 | train MSE: 0.037719 | val MSE (masked): 0.025152 | best: 0.025152\n",
      "  Epoch  10/100 | train MSE: 0.036142 | val MSE (masked): 0.024148 | best: 0.024148\n",
      "  Epoch  11/100 | train MSE: 0.035290 | val MSE (masked): 0.023803 | best: 0.023803\n",
      "  Epoch  12/100 | train MSE: 0.034124 | val MSE (masked): 0.022769 | best: 0.022769\n",
      "  Epoch  13/100 | train MSE: 0.033861 | val MSE (masked): 0.022127 | best: 0.022127\n",
      "  Epoch  14/100 | train MSE: 0.033295 | val MSE (masked): 0.021978 | best: 0.021978\n",
      "  Epoch  15/100 | train MSE: 0.032148 | val MSE (masked): 0.021576 | best: 0.021576\n",
      "  Epoch  16/100 | train MSE: 0.031505 | val MSE (masked): 0.022057 | best: 0.021576\n",
      "  Epoch  17/100 | train MSE: 0.031152 | val MSE (masked): 0.021716 | best: 0.021576\n",
      "  Epoch  18/100 | train MSE: 0.030393 | val MSE (masked): 0.022335 | best: 0.021576\n",
      "  Epoch  19/100 | train MSE: 0.029699 | val MSE (masked): 0.022732 | best: 0.021576\n",
      "  Epoch  20/100 | train MSE: 0.029343 | val MSE (masked): 0.022301 | best: 0.021576\n",
      "  Epoch  21/100 | train MSE: 0.028774 | val MSE (masked): 0.021920 | best: 0.021576\n",
      "  Epoch  22/100 | train MSE: 0.028198 | val MSE (masked): 0.022615 | best: 0.021576\n",
      "  Epoch  23/100 | train MSE: 0.027785 | val MSE (masked): 0.021271 | best: 0.021271\n",
      "  Epoch  24/100 | train MSE: 0.026387 | val MSE (masked): 0.021287 | best: 0.021271\n",
      "  Epoch  25/100 | train MSE: 0.026601 | val MSE (masked): 0.021260 | best: 0.021260\n",
      "  Epoch  26/100 | train MSE: 0.025955 | val MSE (masked): 0.021705 | best: 0.021260\n",
      "  Epoch  27/100 | train MSE: 0.025557 | val MSE (masked): 0.022222 | best: 0.021260\n",
      "  Epoch  28/100 | train MSE: 0.025288 | val MSE (masked): 0.021917 | best: 0.021260\n",
      "  Epoch  29/100 | train MSE: 0.023766 | val MSE (masked): 0.021695 | best: 0.021260\n",
      "  Epoch  30/100 | train MSE: 0.024429 | val MSE (masked): 0.021891 | best: 0.021260\n",
      "  Epoch  31/100 | train MSE: 0.023658 | val MSE (masked): 0.022490 | best: 0.021260\n",
      "  Epoch  32/100 | train MSE: 0.023249 | val MSE (masked): 0.022920 | best: 0.021260\n",
      "  Epoch  33/100 | train MSE: 0.023165 | val MSE (masked): 0.022676 | best: 0.021260\n",
      "  Epoch  34/100 | train MSE: 0.022093 | val MSE (masked): 0.022589 | best: 0.021260\n",
      "  Epoch  35/100 | train MSE: 0.022153 | val MSE (masked): 0.022202 | best: 0.021260\n",
      "  Epoch  36/100 | train MSE: 0.021566 | val MSE (masked): 0.023001 | best: 0.021260\n",
      "  Epoch  37/100 | train MSE: 0.020851 | val MSE (masked): 0.022955 | best: 0.021260\n",
      "  Epoch  38/100 | train MSE: 0.020837 | val MSE (masked): 0.022727 | best: 0.021260\n",
      "  Epoch  39/100 | train MSE: 0.019966 | val MSE (masked): 0.022584 | best: 0.021260\n",
      "  Epoch  40/100 | train MSE: 0.020540 | val MSE (masked): 0.022588 | best: 0.021260\n",
      "  Epoch  41/100 | train MSE: 0.020233 | val MSE (masked): 0.021749 | best: 0.021260\n",
      "  Epoch  42/100 | train MSE: 0.020327 | val MSE (masked): 0.022189 | best: 0.021260\n",
      "  Epoch  43/100 | train MSE: 0.019842 | val MSE (masked): 0.022374 | best: 0.021260\n",
      "  Epoch  44/100 | train MSE: 0.019370 | val MSE (masked): 0.022449 | best: 0.021260\n",
      "  Epoch  45/100 | train MSE: 0.019842 | val MSE (masked): 0.022988 | best: 0.021260\n",
      "  Epoch  46/100 | train MSE: 0.019318 | val MSE (masked): 0.023006 | best: 0.021260\n",
      "  Epoch  47/100 | train MSE: 0.019553 | val MSE (masked): 0.023182 | best: 0.021260\n",
      "  Epoch  48/100 | train MSE: 0.018671 | val MSE (masked): 0.023030 | best: 0.021260\n",
      "  Epoch  49/100 | train MSE: 0.018896 | val MSE (masked): 0.022893 | best: 0.021260\n",
      "  Epoch  50/100 | train MSE: 0.018732 | val MSE (masked): 0.022708 | best: 0.021260\n",
      "  Epoch  51/100 | train MSE: 0.018160 | val MSE (masked): 0.022847 | best: 0.021260\n",
      "  Epoch  52/100 | train MSE: 0.018515 | val MSE (masked): 0.023101 | best: 0.021260\n",
      "  Epoch  53/100 | train MSE: 0.018195 | val MSE (masked): 0.022864 | best: 0.021260\n",
      "  Epoch  54/100 | train MSE: 0.018554 | val MSE (masked): 0.022518 | best: 0.021260\n",
      "  Epoch  55/100 | train MSE: 0.018235 | val MSE (masked): 0.022944 | best: 0.021260\n",
      "  Epoch  56/100 | train MSE: 0.018634 | val MSE (masked): 0.022970 | best: 0.021260\n",
      "  Epoch  57/100 | train MSE: 0.018249 | val MSE (masked): 0.022891 | best: 0.021260\n",
      "  Epoch  58/100 | train MSE: 0.018450 | val MSE (masked): 0.022883 | best: 0.021260\n",
      "  Epoch  59/100 | train MSE: 0.017947 | val MSE (masked): 0.023036 | best: 0.021260\n",
      "  Epoch  60/100 | train MSE: 0.018183 | val MSE (masked): 0.023083 | best: 0.021260\n",
      "  Epoch  61/100 | train MSE: 0.018033 | val MSE (masked): 0.023149 | best: 0.021260\n",
      "  Epoch  62/100 | train MSE: 0.017623 | val MSE (masked): 0.022818 | best: 0.021260\n",
      "  Epoch  63/100 | train MSE: 0.017898 | val MSE (masked): 0.022845 | best: 0.021260\n",
      "  Epoch  64/100 | train MSE: 0.018193 | val MSE (masked): 0.023029 | best: 0.021260\n",
      "  Epoch  65/100 | train MSE: 0.018062 | val MSE (masked): 0.023119 | best: 0.021260\n",
      "  Epoch  66/100 | train MSE: 0.017200 | val MSE (masked): 0.022779 | best: 0.021260\n",
      "  Epoch  67/100 | train MSE: 0.017873 | val MSE (masked): 0.022655 | best: 0.021260\n",
      "  Epoch  68/100 | train MSE: 0.017878 | val MSE (masked): 0.022568 | best: 0.021260\n",
      "  Epoch  69/100 | train MSE: 0.017719 | val MSE (masked): 0.022923 | best: 0.021260\n",
      "  Epoch  70/100 | train MSE: 0.017518 | val MSE (masked): 0.022553 | best: 0.021260\n",
      "  Epoch  71/100 | train MSE: 0.017704 | val MSE (masked): 0.022997 | best: 0.021260\n",
      "  Epoch  72/100 | train MSE: 0.017497 | val MSE (masked): 0.022612 | best: 0.021260\n",
      "  Epoch  73/100 | train MSE: 0.017039 | val MSE (masked): 0.022806 | best: 0.021260\n",
      "  Epoch  74/100 | train MSE: 0.017710 | val MSE (masked): 0.022808 | best: 0.021260\n",
      "  Epoch  75/100 | train MSE: 0.017303 | val MSE (masked): 0.022591 | best: 0.021260\n",
      "  Epoch  76/100 | train MSE: 0.017361 | val MSE (masked): 0.022713 | best: 0.021260\n",
      "  Epoch  77/100 | train MSE: 0.017471 | val MSE (masked): 0.022569 | best: 0.021260\n",
      "  Epoch  78/100 | train MSE: 0.017535 | val MSE (masked): 0.022809 | best: 0.021260\n",
      "  Epoch  79/100 | train MSE: 0.017376 | val MSE (masked): 0.022694 | best: 0.021260\n",
      "  Epoch  80/100 | train MSE: 0.017269 | val MSE (masked): 0.022653 | best: 0.021260\n",
      "  Epoch  81/100 | train MSE: 0.017327 | val MSE (masked): 0.022702 | best: 0.021260\n",
      "  Epoch  82/100 | train MSE: 0.017042 | val MSE (masked): 0.022969 | best: 0.021260\n",
      "  Epoch  83/100 | train MSE: 0.017115 | val MSE (masked): 0.022654 | best: 0.021260\n",
      "  Epoch  84/100 | train MSE: 0.017143 | val MSE (masked): 0.022724 | best: 0.021260\n",
      "  Epoch  85/100 | train MSE: 0.017220 | val MSE (masked): 0.022803 | best: 0.021260\n",
      "  Epoch  86/100 | train MSE: 0.017192 | val MSE (masked): 0.022698 | best: 0.021260\n",
      "  Epoch  87/100 | train MSE: 0.016983 | val MSE (masked): 0.022691 | best: 0.021260\n",
      "  Epoch  88/100 | train MSE: 0.017347 | val MSE (masked): 0.022993 | best: 0.021260\n",
      "  Epoch  89/100 | train MSE: 0.017478 | val MSE (masked): 0.023052 | best: 0.021260\n",
      "  Epoch  90/100 | train MSE: 0.017016 | val MSE (masked): 0.023072 | best: 0.021260\n",
      "  Epoch  91/100 | train MSE: 0.017600 | val MSE (masked): 0.023084 | best: 0.021260\n",
      "  Epoch  92/100 | train MSE: 0.016850 | val MSE (masked): 0.023096 | best: 0.021260\n",
      "  Epoch  93/100 | train MSE: 0.016873 | val MSE (masked): 0.023279 | best: 0.021260\n",
      "  Epoch  94/100 | train MSE: 0.017103 | val MSE (masked): 0.022909 | best: 0.021260\n",
      "  Epoch  95/100 | train MSE: 0.016616 | val MSE (masked): 0.023047 | best: 0.021260\n",
      "  Epoch  96/100 | train MSE: 0.017262 | val MSE (masked): 0.023189 | best: 0.021260\n",
      "  Epoch  97/100 | train MSE: 0.016904 | val MSE (masked): 0.023047 | best: 0.021260\n",
      "  Epoch  98/100 | train MSE: 0.017500 | val MSE (masked): 0.023228 | best: 0.021260\n",
      "  Epoch  99/100 | train MSE: 0.017490 | val MSE (masked): 0.023053 | best: 0.021260\n",
      "  Epoch 100/100 | train MSE: 0.017188 | val MSE (masked): 0.023213 | best: 0.021260\n",
      "\n",
      "Restored best model (val MSE: 0.021260)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 9. TRAIN\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10,\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state    = None\n",
    "\n",
    "print(\"\\nTraining (loss computed on masked cells only)...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for xb, yb, idxb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = masked_mse_loss(pred, yb, mask_train, idxb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(xb)\n",
    "\n",
    "    epoch_loss /= len(X_train)\n",
    "\n",
    "    # Validation: MSE on masked cells\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "    val_loss = 0.0\n",
    "    for j in range(len(X_val)):\n",
    "        idx = mask_val[j]\n",
    "        val_loss += ((val_pred[j, idx] - Y_val[j, idx]) ** 2).mean().item()\n",
    "    val_loss /= len(X_val)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"  Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"train MSE: {epoch_loss:.6f} | \"\n",
    "              f\"val MSE (masked): {val_loss:.6f} | \"\n",
    "              f\"best: {best_val_loss:.6f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nRestored best model (val MSE: {best_val_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f850ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "VALIDATION RESULTS — masked cells only (original scale)\n",
      "=======================================================\n",
      "  Overall RMSE : 0.014704\n",
      "  Overall MAE  : 0.010172\n",
      "  (Volatility range ≈ 0.02 – 0.45)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 10. EVALUATE  (on masked cells only, original scale)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred_np = model(X_val).numpy()\n",
    "\n",
    "val_true_np = Y_val.numpy()\n",
    "\n",
    "# Collect only the masked cell predictions and targets\n",
    "all_pred, all_true = [], []\n",
    "for j in range(len(X_val)):\n",
    "    idx = mask_val[j]\n",
    "    pred_orig = surface_scaler.inverse_transform(val_pred_np[j:j+1])[:, idx]\n",
    "    true_orig = surface_scaler.inverse_transform(val_true_np[j:j+1])[:, idx]\n",
    "    all_pred.append(pred_orig.flatten())\n",
    "    all_true.append(true_orig.flatten())\n",
    "\n",
    "all_pred = np.concatenate(all_pred)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "mae  = np.mean(np.abs(all_true - all_pred))\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"VALIDATION RESULTS — masked cells only (original scale)\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Overall RMSE : {rmse:.6f}\")\n",
    "print(f\"  Overall MAE  : {mae:.6f}\")\n",
    "print(f\"  (Volatility range ≈ 0.02 – 0.45)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b577d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo prediction for test-style missing cells:\n",
      "  T5/M0.0833333333333333 → predicted: 0.042142 | true: 0.035875\n",
      "  T15/M0.25            → predicted: 0.063201 | true: 0.058743\n",
      "  T10/M0.5             → predicted: 0.098090 | true: 0.085806\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 11. TEST SET INFERENCE  (how to use at test time)\n",
    "# ─────────────────────────────────────────────\n",
    "#\n",
    "# When you receive the test set with NaN values:\n",
    "#\n",
    "#   test_row = {col: value or NaN}\n",
    "#\n",
    "# Step 1: build the masked surface and binary mask\n",
    "# Step 2: run through PCA pipeline\n",
    "# Step 3: run through model\n",
    "# Step 4: use model predictions ONLY for the NaN positions\n",
    "\n",
    "def predict_missing(row_dict: dict, model, surface_scaler, pca, pca_scaler,\n",
    "                    feature_cols, device):\n",
    "    \"\"\"\n",
    "    row_dict: {column_name: value_or_nan}\n",
    "    Returns: dict of {column_name: predicted_value} for only the missing cells\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    surface   = np.array([row_dict.get(c, np.nan) for c in feature_cols],\n",
    "                          dtype=np.float32)\n",
    "    mask      = (~np.isnan(surface)).astype(np.float32)\n",
    "    missing   = np.where(np.isnan(surface))[0]\n",
    "\n",
    "    # Fill missing with 0 (neutral value in [0,1] scaled space)\n",
    "    surface_filled = surface.copy()\n",
    "    surface_filled[np.isnan(surface_filled)] = 0.0\n",
    "\n",
    "    # Scale observed cells\n",
    "    # We scale the full row — zeros in missing positions won't affect the fit\n",
    "    surface_scaled = surface_scaler.transform(surface_filled[np.newaxis, :])[0]\n",
    "    surface_scaled[missing] = 0.0   # re-zero after scaling\n",
    "\n",
    "    # Build input: [scaled_surface | mask]\n",
    "    inp = np.concatenate([surface_scaled, mask])[np.newaxis, :].astype(np.float32)\n",
    "\n",
    "    # PCA + re-scale\n",
    "    inp_pca = pca.transform(inp)\n",
    "    inp_pca = pca_scaler.transform(inp_pca).astype(np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(torch.tensor(inp_pca, device=device)).numpy()\n",
    "\n",
    "    pred_original = surface_scaler.inverse_transform(pred_scaled)[0]\n",
    "\n",
    "    return {feature_cols[i]: float(pred_original[i]) for i in missing}\n",
    "\n",
    "\n",
    "# Demo on last training row with synthetic missing cells\n",
    "demo_row = {col: float(raw_data[-1, i]) for i, col in enumerate(feature_cols)}\n",
    "# Simulate the test set example: specific cells missing\n",
    "test_missing_cols = [\n",
    "    \"Tenor : 5; Maturity : 0.0833333333333333\",\n",
    "    \"Tenor : 15; Maturity : 0.25\",\n",
    "    \"Tenor : 10; Maturity : 0.5\",\n",
    "]\n",
    "for col in test_missing_cols:\n",
    "    demo_row[col] = np.nan\n",
    "\n",
    "predictions = predict_missing(demo_row, model, surface_scaler, pca, pca_scaler,\n",
    "                               feature_cols, DEVICE)\n",
    "print(f\"\\nDemo prediction for test-style missing cells:\")\n",
    "for col, pred_val in predictions.items():\n",
    "    short   = col.replace(\"Tenor : \", \"T\").replace(\"; Maturity : \", \"/M\")\n",
    "    true_val = raw_data[-1, feature_cols.index(col)]\n",
    "    print(f\"  {short:<20} → predicted: {pred_val:.6f} | true: {true_val:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
